<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My notes</title>
    <link>https://csu-fangjun.github.io/</link>
    <description>Recent content on My notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 19 Jan 2019 10:46:22 +0800</lastBuildDate>
    
        <atom:link href="https://csu-fangjun.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Computer Vision and Robotics</title>
      <link>https://csu-fangjun.github.io/post/computer-vision-and-robotics/</link>
      <pubDate>Sat, 19 Jan 2019 10:46:22 +0800</pubDate>
      
      <guid>https://csu-fangjun.github.io/post/computer-vision-and-robotics/</guid>
      
        <description>

&lt;h3 id=&#34;courses&#34;&gt;Courses&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.inf.ed.ac.uk/teaching/courses/mlp/lectures-2018.html&#34;&gt;Machine learning practical (MLP) 2018-19: lectures&lt;/a&gt;, 2018, the university&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ws2018/ml4cv&#34;&gt;Machine Learning for Computer Vision (IN2357) (2h + 2h, 5ECTS)&lt;/a&gt;, TUM, ws2018&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs231a/index.html&#34;&gt;CS231A: Computer Vision, From 3D Reconstruction to Recognition&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~16385/&#34;&gt;16-385 Computer Vision, Spring 2018&lt;/a&gt;, at CMU&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.psu.edu/~rtc12/CSE586/&#34;&gt;CSE586/EE554 Computer Vision II&lt;/a&gt;, Penn State University, 2015
of Edinburgh, and &lt;a href=&#34;http://www.inf.ed.ac.uk/teaching/courses/mlp/lectures-2017.html&#34;&gt;2017&lt;/a&gt; and &lt;a href=&#34;https://github.com/CSTR-Edinburgh/mlpractical&#34;&gt;github&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www-inst.eecs.berkeley.edu/~ee290t/fa09/&#34;&gt;EE290T, Fall 2009 Advanced Topics in Signal Processing: 3D image processing and Computer Vision&lt;/a&gt;, berkeley&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.math.louisville.edu/~pksaho01/teaching/&#34;&gt;Multiple View Geometry in Computer Vision&lt;/a&gt;, 2009, lecture slides;
its follows the classic book by Prof. Zisserman. lecture1&amp;ndash;lecture23.pdf&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;lecture 2 and lecture 1 are combined into lecture1.pdf&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ss2018/mvg2018&#34;&gt;Computer Vision II: Multiple View Geometry (IN2228)&lt;/a&gt;, mvg underline lowercase SS18&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ws2018/cvvm_ws2018&#34;&gt;Computer Vision I: Variational Methods&lt;/a&gt;, cvvm underline lowercase WS18&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ws2018/visnav_ws2018&#34;&gt;Practical Course: Vision-based Navigation IN2106&lt;/a&gt;, ws18&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.psu.edu/~rtc12/CSE486/&#34;&gt;CSE/EE486 Computer Vision I&lt;/a&gt;, 2012, Penn State University&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.psu.edu/~rtc12/CSE586/indexSpring11.html&#34;&gt;CSE586/EE554 Computer Vision II&lt;/a&gt;, 2011, Penn State University&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.psu.edu/~rtc12/CSE598C/&#34;&gt;CSE598C Vision-Based Tracking&lt;/a&gt;, 2012, Penn State University&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cs.ucf.edu/~bagci/teaching/computervision17.html&#34;&gt;CAP5415-Computer Vision (FALL 2017)&lt;/a&gt;, 2017&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www8.cs.umu.se/kurser/TDBD19/VT05/&#34;&gt;Geometric Image Analysis, VT-05&lt;/a&gt;, 2005, a good summary of the book MVG&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www8.cs.umu.se/kurser/TDBD19/VT05/tentor/checklist-before-exam.pdf&#34;&gt;Checklist before exam on the course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ss2015/autonavx&#34;&gt;Autonomous Navigation for Flying Robots (EdX course, 2 ECTS)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ws2015/visnav_ws2015&#34;&gt;Practical Course: Vision-based Navigation (6h SWS / 10 ECTS)&lt;/a&gt;, visnav underline lowercase ws15&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ss2013/visnav2013&#34;&gt;Visual Navigation for Flying Robots&lt;/a&gt;, no password! 2013&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=z3dW2f7EbjY&amp;amp;index=5&amp;amp;list=PLTBdjV_4f-EKBCUs1HmMtsnXv4JUoFrzg&#34;&gt;Autonomous Navigation for Flying Robots&lt;/a&gt;, youtube&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/16311/www/s18/syllabus/syllabus.html&#34;&gt;16-311 Introduction to Robotics&lt;/a&gt;, CMU, 2018&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/teaching/ss2013/ml_ss13&#34;&gt;Machine Learning for Robotics and Computer Vision&lt;/a&gt;, 2013&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.dis.uniroma1.it/~grisetti/teaching/probabilistic_robotics_2016_17/web/&#34;&gt;Probabilistic Robotics&lt;/a&gt; at Università di Roma, &lt;sup&gt;2016&lt;/sup&gt;&amp;frasl;&lt;sub&gt;17&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;with code&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://courses.cs.washington.edu/courses/cse571/16au/&#34;&gt;CSE 571: Robotics&lt;/a&gt; at Uni Washington, 2016&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;with code&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ss18/robotics/&#34;&gt;Introduction to Mobile Robotics - SS 2018&lt;/a&gt;, Uni Freiburg&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;one of the authors of the book!&lt;/li&gt;
&lt;li&gt;lecture slides/videos/assignments are online available&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws17/mapping/&#34;&gt;Robot Mapping - WS &lt;sup&gt;2017&lt;/sup&gt;&amp;frasl;&lt;sub&gt;18&lt;/sub&gt;&lt;/a&gt; at Uni Freiburg&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and &lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/&#34;&gt;WS18/19&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;computer-graphics&#34;&gt;Computer Graphics&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cg.informatik.uni-freiburg.de/teaching.htm#material&#34;&gt;Image Processing and Computer Graphics&lt;/a&gt;, WS18/19, Uni Freiburg&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;papers&#34;&gt;Papers&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/bb9e/b6581e9d8820c2a10675b81dbd0338c60afb.pdf&#34;&gt;Visual navigation for mobile robots: A survey&lt;/a&gt;, 2008&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Jose_Ascencio/publication/234081012_Visual_Simultaneous_Localization_and_Mapping_A_Survey/links/55383e610cf247b8587d3d58/Visual-Simultaneous-Localization-and-Mapping-A-Survey.pdf&#34;&gt;Visual simultaneous localization and mapping: a survey&lt;/a&gt;, 2015&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Daniel_Asmar/publication/304787224_A_survey_on_non-filter-based_monocular_Visual_SLAM_systems/links/57da713908ae4e6f18424c23.pdf&#34;&gt;A survey on non-filter-based monocular visual SLAM systems&lt;/a&gt;, 2016&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;state-estimation-for-robotics&#34;&gt;State Estimation for Robotics&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;website of the author: &lt;a href=&#34;http://asrl.utias.utoronto.ca/~tdb/&#34;&gt;http://asrl.utias.utoronto.ca/~tdb/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a course taught by the author:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://asrl.utias.utoronto.ca/~tdb/mr&#34;&gt;AER1514: Introduction to Mobile Robotics&lt;/a&gt;, username is lowercase MR and pass is lowercase DALEK&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://asrl.utias.utoronto.ca/~tdb/mr&#34;&gt;AER521: Mobile Robotics and Perception &lt;/a&gt;, save URL with the above one&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>昆剧</title>
      <link>https://csu-fangjun.github.io/draft/kun-ju/</link>
      <pubDate>Sat, 27 Oct 2018 22:05:35 +0800</pubDate>
      
      <guid>https://csu-fangjun.github.io/draft/kun-ju/</guid>
      
        <description>&lt;ul&gt;
&lt;li&gt;牡丹亭&lt;/li&gt;
&lt;li&gt;桃花扇&lt;/li&gt;
&lt;li&gt;朱买臣休妻&lt;/li&gt;
&lt;li&gt;血冤&lt;/li&gt;
&lt;li&gt;白罗衫&lt;/li&gt;
&lt;li&gt;玉簪记&lt;/li&gt;
&lt;li&gt;焚香记&lt;/li&gt;
&lt;li&gt;看钱奴&lt;/li&gt;
&lt;li&gt;绣襦记&lt;/li&gt;
&lt;li&gt;窦娥冤&lt;/li&gt;
&lt;li&gt;小孙屠&lt;/li&gt;
&lt;li&gt;绿牡丹&lt;/li&gt;
&lt;li&gt;1699 桃花扇&lt;/li&gt;
&lt;li&gt;梁祝&lt;/li&gt;
&lt;li&gt;南柯梦&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>https://csu-fangjun.github.io/post/machine-learning/</link>
      <pubDate>Thu, 25 Oct 2018 16:13:09 +0800</pubDate>
      
      <guid>https://csu-fangjun.github.io/post/machine-learning/</guid>
      
        <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;murphy-gaussians: &lt;a href=&#34;http://ais.informatik.uni-freiburg.de/teaching/ws17/mapping/pdf/murphy-gaussians.pdf&#34;&gt;http://ais.informatik.uni-freiburg.de/teaching/ws17/mapping/pdf/murphy-gaussians.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[Masterpraktikum, Deep Learning Lab][33], Uni Freiburg, WS &lt;sup&gt;2018&lt;/sup&gt;&amp;frasl;&lt;sub&gt;19&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;code at github: &lt;a href=&#34;https://github.com/aisrobots/dl-lab-2018&#34;&gt;https://github.com/aisrobots/dl-lab-2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ml.informatik.uni-freiburg.de/former/teaching/ws1617/dl.html&#34;&gt;Deep Learning Course, Control Section&lt;/a&gt;, Uni Freiburg, WS16/17&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;code at github: &lt;a href=&#34;https://github.com/mllfreiburg/dl_lab_2016&#34;&gt;https://github.com/mllfreiburg/dl_lab_2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cs231n.stanford.edu/&#34;&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;, Uni Stanford, 2018&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;a book: &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/index.html&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;free online html&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224n/&#34;&gt;CS224n: Natural Language Processing with Deep Learning&lt;/a&gt;, stanford, 2018&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/affine/node2.html&#34;&gt;The Projective Camera&lt;/a&gt;, note that the pinhole model is only
a special class of the projective camera model&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/affine/node3.html&#34;&gt;The Perspective Camera&lt;/a&gt;, i.e., the pinhole model&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/affine/node5.html&#34;&gt;The Weak-Perspective Camera&lt;/a&gt;, it is similar to the pinhole camera model except that it groups objects in the space with similar depth and replace their depth
with the same value $z$. Thus, during the projection, $\frac{f}{z}$ is the same
for all pixels, which is similar to orthographic projection. When to use this model:
when the object is far away from the camera such that $z &amp;gt;&amp;gt; f$ and the field of view
is small. Parallel lines are preserved during projection.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/affine/node6.html&#34;&gt;The orthographic camera&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;camera-calibration&#34;&gt;Camera Calibration&lt;/h3&gt;

&lt;p&gt;Refer to &lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node39.html&#34;&gt;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node39.html&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node40.html&#34;&gt;Tsai camera model and calibration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node43.html&#34;&gt;Camera calibration and absolute conic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node43.html&#34;&gt;Camera calibration and absolute conic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node44.html&#34;&gt;What does calibration give?&lt;/a&gt;, angle between rays, normal vector
of a plane through the camera center.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node45.html&#34;&gt;The image of the absolute conic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.iitd.ernet.in/~suban/vision/geometry/node46.html&#34;&gt;A simple calibration device&lt;/a&gt;, using absolute conic for calibration.
It needs 5 image of circular points, which are obtained from 3 planes.&lt;/li&gt;
&lt;li&gt;using vanishing points to determine the absolute conic, refer to
the lecture slide &lt;a href=&#34;http://www.vision.is.tohoku.ac.jp/files/2714/9360/0441/2-camera_calibration.pdf&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the orthographic camera model, every object has the same magnification;
while in the weak perspective model, distant object looks smaller. Objects have
similar $z$ have the same magnification factor.&lt;/p&gt;

&lt;p&gt;Weak perspective projection can be considered as a combination of perspective and orthographic
projection.&lt;/p&gt;

&lt;p&gt;Refer to &lt;a href=&#34;https://www.springer.com/cda/content/document/cda_downloaddocument/9780857290458-c2.pdf?SGWID=0-0-45-998549-p174031075&#34;&gt;Simplified Camera Projection Models&lt;/a&gt;, pdf.&lt;/p&gt;

&lt;h3 id=&#34;projective-geometry&#34;&gt;Projective Geometry&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ideal points, points at infinity&lt;/li&gt;
&lt;li&gt;line at infinity, plane at infinity&lt;/li&gt;
&lt;li&gt;circular points, absolute conic&lt;/li&gt;
&lt;li&gt;vanishing points

&lt;ul&gt;
&lt;li&gt;if a point lies on the plane at infinity, then its image is called vanishing point&lt;/li&gt;
&lt;li&gt;two parallel lines intersect at a point on the plane at infinity, so the image
of the intersection is a vanishing point&lt;/li&gt;
&lt;li&gt;it can be used for camera calibration! (1) identify two vanishing points;
(2) identify the angle of the two corresponding lines; (3) we get a constraint;
(4) the intrinsics have 5 degree of freedom, so we need to find five pairs of vanishing points&lt;/li&gt;
&lt;li&gt;by vanishing points, we can use just one image to calibrate an image!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;vanishing line

&lt;ul&gt;
&lt;li&gt;it is the image of the plane at infinity&lt;/li&gt;
&lt;li&gt;all vanishing points lie on the vanishing line&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;image of the absolute conic (IAC)

&lt;ul&gt;
&lt;li&gt;it is useful for camera calibration!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;todo&#34;&gt;todo&lt;/h1&gt;

&lt;p&gt;adaboost, graphical models, SVM
face recognition (pattern matching)&lt;/p&gt;

&lt;p&gt;boosting gbdt xgb lgb, auc area, roc curve, light gbm
random forest, xgboost&lt;/p&gt;

&lt;p&gt;crf (conditional random field)&lt;/p&gt;

&lt;p&gt;word2vec, lstm&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs230/&#34;&gt;CS230: Deep Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~ph/869/papers/Criminisi99.pdf&#34;&gt;Single View Metrology&lt;/a&gt; and
this &lt;a href=&#34;http://www.cs.cmu.edu/~ph/869/src/asst3/asst3.html&#34;&gt;assignment&lt;/a&gt; and
refer to &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/projects/SingleView/otherlinks.html&#34;&gt;this one&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Blog With Hugo</title>
      <link>https://csu-fangjun.github.io/post/blog-with-hugo/</link>
      <pubDate>Tue, 16 Oct 2018 18:28:13 +0800</pubDate>
      
      <guid>https://csu-fangjun.github.io/post/blog-with-hugo/</guid>
      
        <description>

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/keysaim/blogs/blob/master/config.toml&#34;&gt;Sample config&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/theme/hugo-theme-jane/post/jane-theme-preview/&#34;&gt;Jane Theme Preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://keysaim.github.io/post/blog/deploy-hugo-blog-in-github.io/&#34;&gt;如何在github.io搭建Hugo博客站&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://keysaim.github.io/post/2017-08-16-how-to-add-comments/&#34;&gt;如何给自己的博客网站加入评论系统&lt;/a&gt;, &lt;a href=&#34;https://github.com/imsun/gitment&#34;&gt;gitment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;hugo new post/some-notes.md&lt;/code&gt;, it creates &lt;code&gt;content/post/some-notes.md&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edit &lt;code&gt;some-notes.md&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;hugo&lt;/code&gt; to generate webpages&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;hugo server&lt;/code&gt; to view the results&lt;/li&gt;
&lt;li&gt;Commit.&lt;/li&gt;
&lt;/ol&gt;
</description>
      
    </item>
    
  </channel>
</rss>