<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Caffe Notes - My notes</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="fangjun" />
  <meta name="description" content="tools/caffe.cpp The statement ::gflags::ParseCommandLineFlags(pargc, pargv, true); changes argc. if (argc == 2) { limits the number of arguments to two. Options starting with - or -- do not contribute to the number of arguments. So the only valid argument is train, test, device_query or time. train RegisterBrewFunction(train); registers the train handler which is invoked when ./caffe train is used. Accepted options: --solver Example: --solver=models/bvlc_reference_caffenet/solver.prototxt ReadSolverParamsFromTextFileOrDie() ReadProtoFromTextFile() int fd = open(filename," />

  <meta name="keywords" content="notes, programming" />






<meta name="generator" content="Hugo 0.49.2" />


<link rel="canonical" href="https://csu-fangjun.github.io/post/caffe-notes/" />



<link rel="icon" href="/favicon.ico" />










<link href="/dist/jane.min.css?v=2.7.0" rel="stylesheet">




<meta property="og:title" content="Caffe Notes" />
<meta property="og:description" content="tools/caffe.cpp The statement ::gflags::ParseCommandLineFlags(pargc, pargv, true); changes argc. if (argc == 2) { limits the number of arguments to two. Options starting with - or -- do not contribute to the number of arguments. So the only valid argument is train, test, device_query or time. train RegisterBrewFunction(train); registers the train handler which is invoked when ./caffe train is used. Accepted options: --solver Example: --solver=models/bvlc_reference_caffenet/solver.prototxt ReadSolverParamsFromTextFileOrDie() ReadProtoFromTextFile() int fd = open(filename," />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://csu-fangjun.github.io/post/caffe-notes/" /><meta property="article:published_time" content="2018-11-02T11:04:50&#43;08:00"/>
<meta property="article:modified_time" content="2018-11-02T11:04:50&#43;08:00"/>

<meta itemprop="name" content="Caffe Notes">
<meta itemprop="description" content="tools/caffe.cpp The statement ::gflags::ParseCommandLineFlags(pargc, pargv, true); changes argc. if (argc == 2) { limits the number of arguments to two. Options starting with - or -- do not contribute to the number of arguments. So the only valid argument is train, test, device_query or time. train RegisterBrewFunction(train); registers the train handler which is invoked when ./caffe train is used. Accepted options: --solver Example: --solver=models/bvlc_reference_caffenet/solver.prototxt ReadSolverParamsFromTextFileOrDie() ReadProtoFromTextFile() int fd = open(filename,">


<meta itemprop="datePublished" content="2018-11-02T11:04:50&#43;08:00" />
<meta itemprop="dateModified" content="2018-11-02T11:04:50&#43;08:00" />
<meta itemprop="wordCount" content="10496">



<meta itemprop="keywords" content="caffe," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Caffe Notes"/>
<meta name="twitter:description" content="tools/caffe.cpp The statement ::gflags::ParseCommandLineFlags(pargc, pargv, true); changes argc. if (argc == 2) { limits the number of arguments to two. Options starting with - or -- do not contribute to the number of arguments. So the only valid argument is train, test, device_query or time. train RegisterBrewFunction(train); registers the train handler which is invoked when ./caffe train is used. Accepted options: --solver Example: --solver=models/bvlc_reference_caffenet/solver.prototxt ReadSolverParamsFromTextFileOrDie() ReadProtoFromTextFile() int fd = open(filename,"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-127562009-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Notes</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/">Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/categories/">Categories</a>
          
        
      </li>
    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      Notes
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
      <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/">Home</a>
          

        

      </li>
    
      <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/post/">Archives</a>
          

        

      </li>
    
      <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/tags/">Tags</a>
          

        

      </li>
    
      <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://csu-fangjun.github.io/categories/">Categories</a>
          

        

      </li>
    
    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">Caffe Notes</h1>
      
      <div class="post-meta">
        <span class="post-time"> 2018-11-02 </span>
        <div class="post-category">
            <a href="https://csu-fangjun.github.io/categories/development/"> Development </a>
            <a href="https://csu-fangjun.github.io/categories/deep_learning/"> Deep_Learning </a>
            
          </div>
        <span class="more-meta"> 10496 words </span>
          <span class="more-meta"> 21 min read </span>

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Table of Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#tools-caffe-cpp">tools/caffe.cpp</a>
<ul>
<li><a href="#train">train</a>
<ul>
<li><a href="#a-sample-solver-prototxt">A sample solver prototxt</a></li>
<li><a href="#how-is-a-solver-created">How is a solver created?</a></li>
<li><a href="#the-sgd-solver">The SGD solver</a></li>
<li><a href="#gradient-clipping">Gradient Clipping</a></li>
<li><a href="#weight-decay">Weight Decay</a></li>
<li><a href="#learning-rate">Learning Rate</a></li>
<li><a href="#momentum-update">Momentum Update</a></li>
<li><a href="#nesterov-momentum-update">Nesterov Momentum Update</a></li>
<li><a href="#adam-momentum-update">Adam Momentum Update</a></li>
<li><a href="#how-is-a-net-created">How is a net created?</a></li>
<li><a href="#how-to-update-a-net">How to update a net</a>
<ul>
<li><a href="#from-v0-to-v1">From v0 to v1</a></li>
<li><a href="#data-upgrade">Data Upgrade</a></li>
<li><a href="#from-v1-to-v2">From v1 to v2</a></li>
<li><a href="#input-upgrade">Input Upgrade</a></li>
<li><a href="#batchnorm-upgrade">BatchNorm Upgrade</a></li>
</ul></li>
<li><a href="#how-to-avoid-upgrading">How to avoid upgrading</a></li>
</ul></li>
<li><a href="#how-are-layers-connected">How are layers connected</a></li>
<li><a href="#how-to-include-exclude-a-layer">How to include/exclude a layer</a>
<ul>
<li><a href="#phase">Phase</a></li>
<li><a href="#level">Level</a></li>
<li><a href="#stage">Stage</a></li>
</ul></li>
</ul></li>
<li><a href="#data-members-of-the-class-net">Data Members of the Class Net</a></li>
<li><a href="#layer">Layer</a></li>
<li><a href="#base-data-layer">Base Data Layer</a></li>
<li><a href="#baseprefetchingdatalayer">BasePrefetchingDataLayer</a>
<ul>
<li><a href="#internalthread">InternalThread</a></li>
</ul></li>
<li><a href="#datalayer">DataLayer</a></li>
<li><a href="#inputlayer">InputLayer</a></li>
<li><a href="#neuron-layer">Neuron Layer</a></li>
<li><a href="#sigmod-layer">Sigmod Layer</a></li>
<li><a href="#tanh-layer">tanh Layer</a></li>
<li><a href="#relu-layer">ReLu Layer</a></li>
<li><a href="#prelu">PReLu</a></li>
<li><a href="#pooling-layer">Pooling Layer</a></li>
<li><a href="#loss-layer">Loss Layer</a></li>
<li><a href="#negative-log-loss-neg-loss">Negative Log Loss (neg loss)</a></li>
<li><a href="#softmax-layer">Softmax Layer</a></li>
<li><a href="#softmax-with-loss-layer">SoftMax with Loss Layer</a></li>
<li><a href="#differences-between-snapshot-and-weights">Differences Between Snapshot and Weights</a>
<ul>
<li><a href="#snapshot">Snapshot</a></li>
<li><a href="#weights">Weights</a></li>
</ul></li>
<li><a href="#convolution">Convolution</a>
<ul>
<li><a href="#1d-convolution-in-pytorch">1D convolution in Pytorch</a></li>
<li><a href="#dilated-convolution">Dilated Convolution</a></li>
</ul></li>
</ul></li>
<li><a href="#cs231n-notes">CS231N Notes</a>
<ul>
<li><a href="#image-classification-data-driven-approach-k-nearest-neighbor-train-val-test-splits">Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits</a></li>
<li><a href="#linear-classification-support-vector-machine-softmax">Linear classification: Support Vector Machine, Softmax</a></li>
<li><a href="#optimization-stochastic-gradient-descent">Optimization: Stochastic Gradient Descent</a></li>
<li><a href="#backpropagation-intuitions">Backpropagation, Intuitions</a></li>
<li><a href="#neural-networks-part-1-setting-up-the-architecture">Neural Networks Part 1: Setting up the Architecture</a></li>
<li><a href="#neural-networks-part-2-setting-up-the-data-and-the-loss">Neural Networks Part 2: Setting up the Data and the Loss</a></li>
<li><a href="#neural-networks-part-3-learning-and-evaluation">Neural Networks Part 3: Learning and Evaluation</a></li>
<li><a href="#putting-it-together-minimal-neural-network-case-study">Putting it together: Minimal Neural Network Case Study</a></li>
<li><a href="#convolutional-neural-networks-architectures-convolution-pooling-layers">Convolutional Neural Networks: Architectures, Convolution / Pooling Layers</a>
<ul>
<li><a href="#layer-patterns">Layer Patterns</a>
<ul>
<li><a href="#lenet">LeNet</a></li>
</ul></li>
<li><a href="#vgg-16">VGG 16</a></li>
</ul></li>
</ul></li>
<li><a href="#datasets">Datasets</a></li>
<li><a href="#todo">todo</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      

<h2 id="tools-caffe-cpp">tools/caffe.cpp</h2>

<p>The statement <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/common.cpp#L45"><code>::gflags::ParseCommandLineFlags(pargc, pargv, true);</code></a>
changes <code>argc</code>. <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L429-L432"><code>if (argc == 2) {</code></a> limits the number of arguments to two.
Options starting with <code>-</code> or <code>--</code> do not contribute to the number of arguments.
So <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L429-L432">the only valid argument</a> is <code>train</code>, <code>test</code>, <code>device_query</code>
or <code>time</code>.</p>

<h3 id="train">train</h3>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L253"><code>RegisterBrewFunction(train);</code></a> registers the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L166">train</a> handler which is invoked when <code>./caffe train</code> is used.</p>

<p>Accepted options:</p>

<ul>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L33"><code>--solver</code></a>

<ul>
<li>Example: <code>--solver=models/bvlc_reference_caffenet/solver.prototxt</code></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/upgrade_proto.cpp#L1119"><code>ReadSolverParamsFromTextFileOrDie()</code></a></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/io.cpp#L34"><code>ReadProtoFromTextFile()</code></a></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/io.cpp#L35"><code>int fd = open(filename, O_RDONLY);</code></a></li>
<li>therefore, the path to the solver proto file is relative to the current
executable <code>caffe</code>.</li>
</ul></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L44"><code>--snapshot</code></a>

<ul>
<li>Example: <code>--snapshot=models/xxx/train_iter_10000.solverstate</code></li>
<li>If provided, do not provide <code>--weights</code>, otherwise it panics.</li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L236"><code>solver-&gt;Restore(FLAGS_snapshot.c_str());</code></a></li>
</ul></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L46"><code>--weights</code></a>

<ul>
<li>Example: <code>--weights=&quot;model/bvlc_reference_caffenet.caffemodel&quot;</code></li>
<li>Example: <code>--weights=&quot;foo.caffemodel,bar.caffemodel,foobar.caffemodel</code></li>
<li>If provided, do not provide <code>--snapshot</code>, otherwise it panics.</li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L226"><code>solver_param.add_weights(FLAGS_weights);</code></a></li>
</ul></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L41"><code>--stage</code></a>

<ul>
<li>Example: <code>--stage=&quot;foo,bar&quot;</code></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L126"><code>boost::split(stages, FLAGS_stage, boost::is_any_of(&quot;,&quot;));</code></a></li>
<li>Note that multiple stages are separated with <code>,</code></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L178"><code>solver_param.mutable_train_state()-&gt;add_stage(stages[i]);</code></a></li>
</ul></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L39"><code>--level</code></a>

<ul>
<li>Example: <code>--level=0</code></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L176"><code>solver_param.mutable_train_state()-&gt;set_level(FLAGS_level);</code></a></li>
</ul></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L29"><code>--gpu</code></a>

<ul>
<li>Example: <code>--gpu=&quot;0&quot;</code>, to use gpu 0</li>
<li>Example: <code>--gpu=&quot;0,3&quot;</code>, to use gpu 0 and gpu 3</li>
<li>Example: <code>--gpu=all</code>, to use all available gpus</li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L102"><code>boost::split(strings, FLAGS_gpu, boost::is_any_of(&quot;,&quot;));</code></a></li>
<li>When not specified, if solver specifies solver mode and solver mode is GPU

<ul>
<li>if device id is specified, use the specified device id</li>
<li>else use gpu 0</li>
</ul></li>
<li>If specified, values specified in solver proto are ignored</li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L215"><code>Caffe::set_solver_count(gpus.size());</code></a></li>
<li>If multiple GPUs are specified, caffe must be built with <code>USE_NCCL</code> enabled, otherwise
it panics.</li>
</ul></li>
</ul>

<p>Inside the <code>train()</code> function, it first <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L229-L230">creates</a> a solver:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">caffe</span><span class="o">::</span><span class="n">Solver</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span>
      <span class="n">solver</span><span class="p">(</span><span class="n">caffe</span><span class="o">::</span><span class="n">SolverRegistry</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;::</span><span class="n">CreateSolver</span><span class="p">(</span><span class="n">solver_param</span><span class="p">));</span>
</code></pre></td></tr></table>
</div>
</div>
<p>and then calls <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L248"><code>solver-&gt;Solve();</code></a></p>

<h4 id="a-sample-solver-prototxt">A sample solver prototxt</h4>

<p>A sample solver prototxt can be found at <a href="https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_solver.prototxt">examples/mnist/lenet_solver.prototxt</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></pre></td>
<td class="lntd">
<pre class="chroma"># The train/test net protocol buffer definition
net: &#34;examples/mnist/lenet_train_test.prototxt&#34;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policy
lr_policy: &#34;inv&#34;
gamma: 0.0001
power: 0.75
# Display every 100 iterations
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate results
snapshot: 5000
snapshot_prefix: &#34;examples/mnist/lenet&#34;
# solver mode: CPU or GPU
solver_mode: GPU</pre></td></tr></table>
</div>
</div>
<h4 id="how-is-a-solver-created">How is a solver created?</h4>

<ul>
<li><a href="https://github.com/BVLC/caffe/blob/master/include/caffe/solver_factory.hpp">include/caffe/solver_factory.hpp</a></li>
</ul>

<p>The base class is <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/solver.hpp#L42">class Solver</a>; the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/sgd_solvers.hpp#L16">sgd solver</a>
is declared as</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">class</span><span class="err"> </span><span class="nc">SGDSolver</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
</code></pre></td></tr></table>
</div>
</div>
<p>and is registered as <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L373">REGISTER_SOLVER_CLASS(SGD);</a></p>

<p>We can create a solver of type <code>SGDSolver</code> by using its <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/proto/caffe.proto#L216">type</a>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="k">optional</span> <span class="kt">string</span> <span class="n">type</span> <span class="o">=</span> <span class="mi">40</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="s">&#34;SGD&#34;</span><span class="p">];</span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="c1">// https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/solver_factory.hpp#L74
</span><span class="c1"></span>    <span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">type</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">type</span><span class="p">();</span>
    <span class="n">CreatorRegistry</span><span class="o">&amp;</span> <span class="n">registry</span> <span class="o">=</span> <span class="n">Registry</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">registry</span><span class="p">[</span><span class="n">type</span><span class="p">](</span><span class="n">param</span><span class="p">);</span>
</code></pre></td></tr></table>
</div>
</div>
<h4 id="the-sgd-solver">The SGD solver</h4>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/sgd_solvers.hpp#L18">contstructor</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="k">explicit</span> <span class="nf">SGDSolver</span><span class="p">(</span><span class="k">const</span> <span class="n">SolverParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="p">{</span> <span class="n">PreSolve</span><span class="p">();</span> <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Inside the constructor of the base class <code>Solver</code>:
    * <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solver.cpp#L78"><code>InitTrainNet()</code></a>
        - read the net parameter (it can be from one of the four sources).
        - set net phase to <code>TRAIN</code>
        - create a new net from net param
        - load pretrained weights if any using <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/net.cpp#L773">net-&gt;CopyTrainedLayersFrom</a>
            * a layer is unqiuely identified by its <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/proto/caffe.proto#L327">name</a>
            * only <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/proto/caffe.proto#L345">blobs</a> of a layer are copied from the pretrained file</p>

<h4 id="gradient-clipping">Gradient Clipping</h4>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L88">Gradient clipping</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">ClipGradients</span><span class="p">()</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span> <span class="n">clip_gradients</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">clip_gradients</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">clip_gradients</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">net_params</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">learnable_params</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">sumsq_diff</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">net_params</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">sumsq_diff</span> <span class="o">+=</span> <span class="n">net_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">sumsq_diff</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">const</span> <span class="n">Dtype</span> <span class="n">l2norm_diff</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sumsq_diff</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">l2norm_diff</span> <span class="o">&gt;</span> <span class="n">clip_gradients</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">Dtype</span> <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">clip_gradients</span> <span class="o">/</span> <span class="n">l2norm_diff</span><span class="p">;</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Gradient clipping: scaling down gradients (L2 norm &#34;</span>
        <span class="o">&lt;&lt;</span> <span class="n">l2norm_diff</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; &gt; &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">clip_gradients</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;) &#34;</span>
        <span class="o">&lt;&lt;</span> <span class="s">&#34;by scale factor &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">scale_factor</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">net_params</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">net_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">scale_diff</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h4 id="weight-decay">Weight Decay</h4>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L156">Weight decay</a>, refer to <a href="https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate">here</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Regularize</span><span class="p">(</span><span class="kt">int</span> <span class="n">param_id</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">net_params</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">learnable_params</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">net_params_weight_decay</span> <span class="o">=</span>
      <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">params_weight_decay</span><span class="p">();</span>
<span class="p">....</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">net_params_weight_decay</span> <span class="o">=</span>
      <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">params_weight_decay</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">weight_decay</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">();</span>
  <span class="n">string</span> <span class="n">regularization_type</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">regularization_type</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">local_decay</span> <span class="o">=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">net_params_weight_decay</span><span class="p">[</span><span class="n">param_id</span><span class="p">];</span>
<span class="p">....</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">regularization_type</span> <span class="o">==</span> <span class="s">&#34;L2&#34;</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// add weight decay
</span><span class="c1"></span>        <span class="n">caffe_axpy</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span>
            <span class="n">local_decay</span><span class="p">,</span>
            <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span>
            <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">());</span>
      <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">regularization_type</span> <span class="o">==</span> <span class="s">&#34;L1&#34;</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">caffe_cpu_sign</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span>
            <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span>
            <span class="n">temp_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">());</span>
        <span class="n">caffe_axpy</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span>
            <span class="n">local_decay</span><span class="p">,</span>
            <span class="n">temp_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span>
            <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">());</span>
<span class="p">....</span>
</code></pre></td></tr></table>
</div>
</div>
<p>There are two places to set the weight decay:</p>

<ul>
<li>For a network, every layer can set its own decay, which is aggregated
into a net. See the code below</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="k">repeated</span> <span class="n">ParamSpec</span> <span class="n">param</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="o">...</span><span class="err">
</span><span class="err"></span><span class="kd">message</span> <span class="nc">ParamSpec</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="k">optional</span> <span class="kt">float</span> <span class="n">decay_mult</span> <span class="o">=</span> <span class="mi">4</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">];</span></code></pre></td></tr></table>
</div>
</div>
<ul>
<li>For a solver,</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">SolverParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="kt">float</span> <span class="n">weight_decay</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span> <span class="c1">// The weight decay.
</span><span class="c1"></span>  <span class="c1">// regularization types supported: L1 and L2
</span><span class="c1"></span>  <span class="c1">// controlled by weight_decay
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">string</span> <span class="n">regularization_type</span> <span class="o">=</span> <span class="mi">29</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="s">&#34;L2&#34;</span><span class="p">];</span><span class="err">
</span><span class="err"></span><span class="o">...</span></code></pre></td></tr></table>
</div>
</div>
<ul>
<li>the total weight for weight decay is the product of the weight decay
of the solver and the corresponding layer.</li>
</ul>

<p>AlexNet use <code>0.0005*learning_rate=0.0005*0.01=5e-6</code> for weight decay.</p>

<h4 id="learning-rate">Learning Rate</h4>

<p>Every layer can set a learning rate:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="k">repeated</span> <span class="n">ParamSpec</span> <span class="n">param</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="o">...</span><span class="err">
</span><span class="err"></span><span class="kd">message</span> <span class="nc">ParamSpec</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="k">optional</span> <span class="kt">float</span> <span class="n">lr_mult</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">];</span></code></pre></td></tr></table>
</div>
</div>
<p>The actual learning rate is the current learning rate multiplied with <code>lr_mult</code>.</p>

<p>The current learning rate is <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L11">determined</a> by</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="c1">// Return the current learning rate. The currently implemented learning rate
</span><span class="c1">// policies are as follows:
</span><span class="c1">//    - fixed: always return base_lr.
</span><span class="c1">//    - step: return base_lr * gamma ^ (floor(iter / step))
</span><span class="c1">//    - exp: return base_lr * gamma ^ iter
</span><span class="c1">//    - inv: return base_lr * (1 + gamma * iter) ^ (- power)
</span><span class="c1">//    - multistep: similar to step but it allows non uniform steps defined by
</span><span class="c1">//      stepvalue
</span><span class="c1">//    - poly: the effective learning rate follows a polynomial decay, to be
</span><span class="c1">//      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)
</span><span class="c1">//    - sigmoid: the effective learning rate follows a sigmod decay
</span><span class="c1">//      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))
</span><span class="c1">//
</span><span class="c1">// where base_lr, max_iter, gamma, step, stepvalue and power are defined
</span><span class="c1">// in the solver parameter protocol buffer, and iter is the current iteration.
</span><span class="c1"></span><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Dtype</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">GetLearningRate</span><span class="p">()</span> <span class="p">{</span>
</code></pre></td></tr></table>
</div>
</div>
<p><code>GetLearningRate()</code> is used in the momentum update.</p>

<h4 id="momentum-update">Momentum Update</h4>

<p>Refer to <a href="https://distill.pub/2017/momentum/">here</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">SolverParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="k">optional</span> <span class="kt">float</span> <span class="n">momentum</span> <span class="o">=</span> <span class="mi">11</span><span class="p">;</span> <span class="o">//</span> <span class="n">The</span> <span class="n">momentum</span> <span class="n">value.</span></code></pre></td></tr></table>
</div>
</div>
<p>See the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L224">code</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">ComputeUpdateValue</span><span class="p">(</span><span class="kt">int</span> <span class="n">param_id</span><span class="p">,</span> <span class="n">Dtype</span> <span class="n">rate</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">net_params</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">learnable_params</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">net_params_lr</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">params_lr</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">momentum</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">momentum</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">local_rate</span> <span class="o">=</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">net_params_lr</span><span class="p">[</span><span class="n">param_id</span><span class="p">];</span>
  <span class="c1">// Compute the update to history, then copy it to the parameter diff.
</span><span class="c1"></span>  <span class="k">switch</span> <span class="p">(</span><span class="n">Caffe</span><span class="o">::</span><span class="n">mode</span><span class="p">())</span> <span class="p">{</span>
  <span class="k">case</span> <span class="n">Caffe</span><span class="o">::</span><span class="nl">CPU</span><span class="p">:</span> <span class="p">{</span>
    <span class="n">caffe_cpu_axpby</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">local_rate</span><span class="p">,</span>
              <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">(),</span> <span class="n">momentum</span><span class="p">,</span>
              <span class="n">history_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">());</span>
    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span>
        <span class="n">history_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span>
        <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">());</span>
    <span class="k">break</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span></pre></td>
<td class="lntd">
<pre class="chroma">last_time = momentum*last_time + learning_rate*current_computed
current_computed = last_time</pre></td></tr></table>
</div>
</div>
<p>(Note that weight decay has been applied to <code>current_computed</code> in a previous step.)</p>

<h4 id="nesterov-momentum-update">Nesterov Momentum Update</h4>

<p>See the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/nesterov_solver.cpp#L13">code</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">NesterovSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">ComputeUpdateValue</span><span class="p">(</span><span class="kt">int</span> <span class="n">param_id</span><span class="p">,</span> <span class="n">Dtype</span> <span class="n">rate</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">net_params</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">learnable_params</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span> <span class="n">net_params_lr</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">params_lr</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">momentum</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">momentum</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">local_rate</span> <span class="o">=</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">net_params_lr</span><span class="p">[</span><span class="n">param_id</span><span class="p">];</span>
  <span class="k">switch</span> <span class="p">(</span><span class="n">Caffe</span><span class="o">::</span><span class="n">mode</span><span class="p">())</span> <span class="p">{</span>
  <span class="k">case</span> <span class="n">Caffe</span><span class="o">::</span><span class="nl">CPU</span><span class="p">:</span> <span class="p">{</span>
    <span class="c1">// save history momentum for stepping back
</span><span class="c1"></span>    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">history_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">update_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">());</span>

    <span class="c1">// update history
</span><span class="c1"></span>    <span class="n">caffe_cpu_axpby</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">local_rate</span><span class="p">,</span>
              <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">(),</span> <span class="n">momentum</span><span class="p">,</span>
              <span class="k">this</span><span class="o">-&gt;</span><span class="n">history_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">());</span>

    <span class="c1">// compute update: step back then over step
</span><span class="c1"></span>    <span class="n">caffe_cpu_axpby</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">momentum</span><span class="p">,</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">history_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span> <span class="o">-</span><span class="n">momentum</span><span class="p">,</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">update_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">());</span>

    <span class="c1">// copy
</span><span class="c1"></span>    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span>
        <span class="k">this</span><span class="o">-&gt;</span><span class="n">update_</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">(),</span>
        <span class="n">net_params</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">());</span>
    <span class="k">break</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">saved = last_time
last_time = momentum*last_time + learning_rate*current_computed
current_computed = (1+momentum)*last_time - momentum*saved</pre></td></tr></table>
</div>
</div>
<h4 id="adam-momentum-update">Adam Momentum Update</h4>

<p>See the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/adam_solver.cpp#L26">code</a></p>

<h4 id="how-is-a-net-created">How is a net created?</h4>

<p>All parameters to be learned are saved in <code>Net::learnable_params()</code></p>

<ul>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/upgrade_proto.cpp#L88"><code>ReadNetParamsFromTextFileOrDie()</code></a>

<ul>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/io.cpp#L34"><code>ReadProtoFromTextFile()</code></a></li>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/upgrade_proto.cpp#L23"><code>UpgradeNetAsNeeded()</code></a></li>
</ul></li>
</ul>

<h4 id="how-to-update-a-net">How to update a net</h4>

<p>There are three versions: v0, v1 and v2.</p>

<h5 id="from-v0-to-v1">From v0 to v1</h5>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">NetParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// DEPRECATED. See InputParameter. The input blobs to the network.
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="kt">string</span> <span class="n">input</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="c1">// The layers that make up the net.  Each of their configurations, including
</span><span class="c1"></span>  <span class="c1">// connectivity and behavior, is specified as a LayerParameter.
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="n">LayerParameter</span> <span class="n">layer</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>  <span class="c1">// ID 100 so layers are printed last.
</span><span class="c1"></span><span class="err">
</span><span class="err"></span>  <span class="c1">// DEPRECATED: use &#39;layer&#39; instead.
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="n">V1LayerParameter</span> <span class="n">layers</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="o">...</span><span class="err">
</span><span class="err">
</span><span class="err"></span><span class="kd">message</span> <span class="nc">V1LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="k">optional</span> <span class="n">V0LayerParameter</span> <span class="n">layer</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span></code></pre></td></tr></table>
</div>
</div>
<p>when <code>layers</code> is set and <code>V0LayerParameter layer</code> is present in
<code>layers</code>, then we need to upgrade from V0 to V1.</p>

<h5 id="data-upgrade">Data Upgrade</h5>

<p>When <code>V1LayerParameter</code> is presented and its type is data, image data or window data.</p>

<h5 id="from-v1-to-v2">From v1 to v2</h5>

<p>If <code>V1LayerParameter</code> is presented, then it is performed.</p>

<h5 id="input-upgrade">Input Upgrade</h5>

<p>It is performed when <code>input</code> is present.</p>

<p>A <code>LayerParameter</code> of type is <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/upgrade_proto.cpp#L977">added</a>.</p>

<h5 id="batchnorm-upgrade">BatchNorm Upgrade</h5>

<p>If <code>LayerParameter</code> is of type <code>BatchNorm</code> and it has 3 parameters:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="kt">string</span> <span class="n">type</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> <span class="c1">// the layer type, BatchNorm
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="n">ParamSpec</span> <span class="n">param</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span> <span class="o">//</span> <span class="n">and</span> <span class="n">if</span> <span class="n">it</span> <span class="n">has</span> <span class="mi">3</span> <span class="n">parameters</span></code></pre></td></tr></table>
</div>
</div>
<p>See the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/upgrade_proto.cpp#L1017">code</a> below:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="kt">void</span> <span class="nf">UpgradeNetBatchNorm</span><span class="p">(</span><span class="n">NetParameter</span><span class="o">*</span> <span class="n">net_param</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">net_param</span><span class="o">-&gt;</span><span class="n">layer_size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Check if BatchNorm layers declare three parameters, as required by
</span><span class="c1"></span>    <span class="c1">// the previous BatchNorm layer definition.
</span><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">net_param</span><span class="o">-&gt;</span><span class="n">layer</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="s">&#34;BatchNorm&#34;</span>
        <span class="o">&amp;&amp;</span> <span class="n">net_param</span><span class="o">-&gt;</span><span class="n">layer</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">param_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// set lr_mult and decay_mult to zero. leave all other param intact.
</span><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ip</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ip</span> <span class="o">&lt;</span> <span class="n">net_param</span><span class="o">-&gt;</span><span class="n">layer</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">param_size</span><span class="p">();</span> <span class="n">ip</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ParamSpec</span><span class="o">*</span> <span class="n">fixed_param_spec</span> <span class="o">=</span>
          <span class="n">net_param</span><span class="o">-&gt;</span><span class="n">mutable_layer</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">mutable_param</span><span class="p">(</span><span class="n">ip</span><span class="p">);</span>
        <span class="n">fixed_param_spec</span><span class="o">-&gt;</span><span class="n">set_lr_mult</span><span class="p">(</span><span class="mf">0.f</span><span class="p">);</span>
        <span class="n">fixed_param_spec</span><span class="o">-&gt;</span><span class="n">set_decay_mult</span><span class="p">(</span><span class="mf">0.f</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>It sets the learning rate and weight decay to 0.</p>

<h4 id="how-to-avoid-upgrading">How to avoid upgrading</h4>

<ol>
<li>Do not use V0LayerParameter</li>
<li>Do not use V1LayerParameter</li>
<li>Do not use <code>input</code>.</li>
<li>Use LayerParameter

<ul>
<li>upgrade is unavoided if a layer type is <code>BatchNorm</code></li>
</ul></li>
</ol>

<h3 id="how-are-layers-connected">How are layers connected</h3>

<ul>
<li><code>Net::Net()</code> calls <code>ReadNetParamsFromTextFileOrDie()</code>
to load <code>NetParameter</code> from a proto txt file.</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="k">explicit</span> <span class="nf">Net</span><span class="p">(</span><span class="k">const</span> <span class="n">NetParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">);</span>
  <span class="k">explicit</span> <span class="nf">Net</span><span class="p">(</span><span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">param_file</span><span class="p">,</span> <span class="n">Phase</span> <span class="n">phase</span><span class="p">,</span>
      <span class="k">const</span> <span class="kt">int</span> <span class="n">level</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;*</span> <span class="n">stages</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">);</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Net</span><span class="p">(</span><span class="k">const</span> <span class="n">NetParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Init</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Net</span><span class="p">(</span><span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">param_file</span><span class="p">,</span> <span class="n">Phase</span> <span class="n">phase</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">level</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;*</span> <span class="n">stages</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">NetParameter</span> <span class="n">param</span><span class="p">;</span>
  <span class="n">ReadNetParamsFromTextFileOrDie</span><span class="p">(</span><span class="n">param_file</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">param</span><span class="p">);</span>
  <span class="c1">// Set phase, stages and level
</span><span class="c1"></span>  <span class="n">param</span><span class="p">.</span><span class="n">mutable_state</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">set_phase</span><span class="p">(</span><span class="n">phase</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">stages</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">stages</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">param</span><span class="p">.</span><span class="n">mutable_state</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">add_stage</span><span class="p">((</span><span class="o">*</span><span class="n">stages</span><span class="p">)[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">param</span><span class="p">.</span><span class="n">mutable_state</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">set_level</span><span class="p">(</span><span class="n">level</span><span class="p">);</span>
  <span class="n">Init</span><span class="p">(</span><span class="n">param</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">enum</span> <span class="n">Phase</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>   <span class="n">TRAIN</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="err">
</span><span class="err"></span>   <span class="n">TEST</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="p">}</span><span class="err">
</span><span class="err">
</span><span class="err"></span><span class="kd">message</span> <span class="nc">NetState</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="n">Phase</span> <span class="n">phase</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="n">TEST</span><span class="p">];</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="kt">int32</span> <span class="n">level</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mi">0</span><span class="p">];</span><span class="err">
</span><span class="err"></span>  <span class="k">repeated</span> <span class="kt">string</span> <span class="n">stage</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>If we have a proto txt file, we can first read it into <code>NetParameter</code>,
set <code>NetState</code>, and then call <code>Net::Net(const string&amp; filename, ...)</code></p>

<ul>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/net.cpp#L46"><code>Net::Init()</code></a>

<ul>
<li><code>FilterNet()</code> to exclude layers using</li>
</ul></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// The train / test phase for computation.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="n">Phase</span> <span class="n">phase</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Rules controlling whether and when a layer is included in the network,
</span><span class="c1"></span>  <span class="c1">// based on the current NetState.  You may specify a non-zero number of rules
</span><span class="c1"></span>  <span class="c1">// to include OR exclude, but not both.  If no include or exclude rules are
</span><span class="c1"></span>  <span class="c1">// specified, the layer is always included.  If the current NetState meets
</span><span class="c1"></span>  <span class="c1">// ANY (i.e., one or more) of the specified rules, the layer is
</span><span class="c1"></span>  <span class="c1">// included/excluded.
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="n">NetStateRule</span> <span class="n">include</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="k">repeated</span> <span class="n">NetStateRule</span> <span class="n">exclude</span> <span class="o">=</span> <span class="mi">9</span><span class="p">;</span><span class="err">
</span><span class="err">
</span><span class="err"></span><span class="o">...</span><span class="err">
</span><span class="err">
</span><span class="err"></span><span class="kd">message</span> <span class="nc">NetStateRule</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Set phase to require the NetState have a particular phase (TRAIN or TEST)
</span><span class="c1"></span>  <span class="c1">// to meet this rule.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="n">Phase</span> <span class="n">phase</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Set the minimum and/or maximum levels in which the layer should be used.
</span><span class="c1"></span>  <span class="c1">// Leave undefined to meet the rule regardless of level.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">int32</span> <span class="n">min_level</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="kt">int32</span> <span class="n">max_level</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Customizable sets of stages to include or exclude.
</span><span class="c1"></span>  <span class="c1">// The net must have ALL of the specified stages and NONE of the specified
</span><span class="c1"></span>  <span class="c1">// &#34;not_stage&#34;s to meet the rule.
</span><span class="c1"></span>  <span class="c1">// (Use multiple NetStateRules to specify conjunctions of stages.)
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="kt">string</span> <span class="n">stage</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="k">repeated</span> <span class="kt">string</span> <span class="n">not_stage</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>We can set either <code>include</code> or <code>exclude</code> or none. If <code>include</code> and
<code>exclude</code> are both set, then it panics.</p>

<ul>
<li><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/insert_splits.cpp#L12"><code>InsertSplits()</code></a></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="kt">string</span> <span class="n">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="c1">// the layer name
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">string</span> <span class="n">type</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> <span class="c1">// the layer type
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="kt">string</span> <span class="n">bottom</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span> <span class="c1">// the name of each bottom blob
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="kt">string</span> <span class="n">top</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span> <span class="c1">// the name of each top blob
</span><span class="c1"></span><span class="err">
</span><span class="err"></span>  <span class="c1">// The amount of weight to assign each top blob in the objective.
</span><span class="c1"></span>  <span class="c1">// Each layer assigns a default value, usually of either 0 or 1,
</span><span class="c1"></span>  <span class="c1">// to each top blob.
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="kt">float</span> <span class="n">loss_weight</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span></code></pre></td></tr></table>
</div>
</div>
<ul>
<li>connect layers

<ul>
<li>if a layer does not have phase specified, then set it to the net phase</li>
<li>if <code>propagate_down</code> is specified, check that it equals to the number of bottoms of this layer</li>
<li><code>layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));</code></li>
</ul></li>
</ul>

<h3 id="how-to-include-exclude-a-layer">How to include/exclude a layer</h3>

<p>We can specify some properties of the net and if the layer
does not have any relevant properties, then the layer is included
by default. If we want to exclude the layer, we have to do
some additional work by specifying its <code>exclude</code> to match
the given properties of the net. If any one of the properties
is satisfied, the layer is excluded.</p>

<p>Another way is to specify the <code>include</code> property of the layer
and if the net does not have the specified property required
by the layer, the layer is not included.</p>

<p><code>include</code> and <code>exclude</code> cannot be specified at the same time;
otherwise it panics.</p>

<h4 id="phase">Phase</h4>

<ul>
<li>Set the phase of the net to <code>TRAIN</code>

<ul>
<li>do nothing to include the layer!</li>
<li>to exclude the layer: set the phase for the <code>exclude</code> in <code>LayerParameter</code>
to <code>TRAIN</code></li>
<li>Note that <code>LayerParameter.Phase</code> is not considered here!</li>
</ul></li>
</ul>

<h4 id="level">Level</h4>

<ul>
<li>Set the level of the net to some number, e.g., 10

<ul>
<li>do nothing to include the layer</li>
<li>to exclude the layer: set the level for the <code>exclude</code> in <code>LayerParameter</code> to

<ul>
<li><code>min_level</code> : e.g., 11, since the level of the net is 10, which is less than 11, so the layer is excluded</li>
<li><code>max_level</code>: e.g., 8, since the level of the net is 10, which is larger than 8, so the layer is excluded</li>
<li>if either <code>min_level</code> or <code>max_level</code> does not meet the level of the net,the layer is excluded</li>
</ul></li>
</ul></li>
</ul>

<h4 id="stage">Stage</h4>

<ul>
<li>Set the stage of the net to some list of strings, e.g., <code>{&quot;foo&quot;, &quot;bar&quot;}</code>

<ul>
<li>do nothing to include the layer</li>
<li>add either &ldquo;foo&rdquo; or &ldquo;bar&rdquo; or both to <code>exclude</code> of the layer</li>
</ul></li>
</ul>

<h2 id="data-members-of-the-class-net">Data Members of the Class Net</h2>

<ul>
<li><code>vector&lt;string&gt; layer_names_;</code>

<ul>
<li><code>layer_names_[0]</code> is the name of the 0th layer</li>
<li><code>layer_names_[1]</code> is the name of the 1st layer</li>
<li>and so on</li>
</ul></li>
<li><code>vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_</code>

<ul>
<li>pointers of all blobs</li>
</ul></li>
<li><code>vector&lt;Blob&lt;Dtype&gt;*&gt; net_input_blobs_</code>

<ul>
<li>saves the pointers of input blobs, which are also saved in <code>blobs_</code></li>
</ul></li>
<li><code>vector&lt;int&gt; net_input_blob_indices_;</code>

<ul>
<li><code>net_input_blob_indices_[0]</code>: the blob <code>blobs_[net_input_blob_indices[0]]</code></li>
<li><code>net_input_blob_indices_[1]</code>: the blob <code>blobs_[net_input_blob_indices[1]]</code></li>
</ul></li>
</ul>

<h2 id="layer">Layer</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">LayerParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">repeated</span> <span class="n">BlobProto</span> <span class="n">blobs</span> <span class="o">=</span> <span class="mi">7</span><span class="p">;</span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">blobs_</span><span class="p">;</span>

  <span class="k">explicit</span> <span class="nf">Layer</span><span class="p">(</span><span class="k">const</span> <span class="n">LayerParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
    <span class="o">:</span> <span class="n">layer_param_</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// Set phase and copy blobs (if there are any).
</span><span class="c1"></span>      <span class="n">phase_</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">phase</span><span class="p">();</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">blobs_</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">());</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">blobs_</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">());</span>
          <span class="n">blobs_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">FromProto</span><span class="p">(</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">blobs</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Setup the internal blobs of a layer in the constructor.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="kt">void</span> <span class="nf">SetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">CheckBlobCounts</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
    <span class="n">LayerSetUp</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
    <span class="n">Reshape</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
    <span class="n">SetLossWeights</span><span class="p">(</span><span class="n">top</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">LayerSetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{}</span>

  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="base-data-layer">Base Data Layer</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">BaseDataLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">LayerSetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">top</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">output_labels_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">output_labels_</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">data_transformer_</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span>
      <span class="k">new</span> <span class="n">DataTransformer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">transform_param_</span><span class="p">,</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">phase_</span><span class="p">));</span>
  <span class="n">data_transformer_</span><span class="o">-&gt;</span><span class="n">InitRand</span><span class="p">();</span>
  <span class="c1">// The subclasses should setup the size of bottom and top
</span><span class="c1"></span>  <span class="n">DataLayerSetUp</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
<span class="p">}</span>

  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">DataLayerSetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{}</span>

  <span class="c1">// Data layers have no bottoms, so reshaping is trivial.
</span><span class="c1"></span>  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{}</span>

  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{}</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">Backward_gpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Subclasses of <code>BaseDataLayer</code> have to implement <code>DataLayerSetUp()</code>.</p>

<p>Note that the 0th blob of the data layer cannot be labels!</p>

<h2 id="baseprefetchingdatalayer">BasePrefetchingDataLayer</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="c1">// base_data_layer.hpp
</span><span class="c1"></span>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">BasePrefetchingDataLayer</span> <span class="o">:</span>
    <span class="k">public</span> <span class="n">BaseDataLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">,</span> <span class="k">public</span> <span class="n">InternalThread</span> <span class="p">{</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">BasePrefetchingDataLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">prefetch_current_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">prefetch_free_</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">prefetch_current_</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">prefetch_current_</span> <span class="o">=</span> <span class="n">prefetch_full_</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s">&#34;Waiting for data&#34;</span><span class="p">);</span>
  <span class="c1">// Reshape to loaded data.
</span><span class="c1"></span>  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ReshapeLike</span><span class="p">(</span><span class="n">prefetch_current_</span><span class="o">-&gt;</span><span class="n">data_</span><span class="p">);</span>
  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">set_cpu_data</span><span class="p">(</span><span class="n">prefetch_current_</span><span class="o">-&gt;</span><span class="n">data_</span><span class="p">.</span><span class="n">mutable_cpu_data</span><span class="p">());</span>
  <span class="k">if</span> <span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">output_labels_</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Reshape to loaded labels.
</span><span class="c1"></span>    <span class="n">top</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ReshapeLike</span><span class="p">(</span><span class="n">prefetch_current_</span><span class="o">-&gt;</span><span class="n">label_</span><span class="p">);</span>
    <span class="n">top</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">set_cpu_data</span><span class="p">(</span><span class="n">prefetch_current_</span><span class="o">-&gt;</span><span class="n">label_</span><span class="p">.</span><span class="n">mutable_cpu_data</span><span class="p">());</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h3 id="internalthread">InternalThread</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">boost</span><span class="o">::</span><span class="kr">thread</span><span class="o">&gt;</span> <span class="n">thread_</span><span class="p">;</span>

  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">InternalThreadEntry</span><span class="p">()</span> <span class="p">{}</span>
<span class="p">...</span>

  <span class="k">try</span> <span class="p">{</span>
    <span class="n">thread_</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">boost</span><span class="o">::</span><span class="kr">thread</span><span class="p">(</span><span class="o">&amp;</span><span class="n">InternalThread</span><span class="o">::</span><span class="n">entry</span><span class="p">,</span> <span class="k">this</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span>
          <span class="n">rand_seed</span><span class="p">,</span> <span class="n">solver_count</span><span class="p">,</span> <span class="n">solver_rank</span><span class="p">,</span> <span class="n">multiprocess</span><span class="p">));</span>
  <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Thread exception: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">e</span><span class="p">.</span><span class="n">what</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">...</span>
<span class="kt">void</span> <span class="n">InternalThread</span><span class="o">::</span><span class="n">entry</span><span class="p">(</span><span class="kt">int</span> <span class="n">device</span><span class="p">,</span> <span class="n">Caffe</span><span class="o">::</span><span class="n">Brew</span> <span class="n">mode</span><span class="p">,</span> <span class="kt">int</span> <span class="n">rand_seed</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">solver_count</span><span class="p">,</span> <span class="kt">int</span> <span class="n">solver_rank</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">multiprocess</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#ifndef CPU_ONLY
</span><span class="cp"></span>  <span class="n">CUDA_CHECK</span><span class="p">(</span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">device</span><span class="p">));</span>
<span class="cp">#endif
</span><span class="cp"></span>  <span class="n">Caffe</span><span class="o">::</span><span class="n">set_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">);</span>
  <span class="n">Caffe</span><span class="o">::</span><span class="n">set_random_seed</span><span class="p">(</span><span class="n">rand_seed</span><span class="p">);</span>
  <span class="n">Caffe</span><span class="o">::</span><span class="n">set_solver_count</span><span class="p">(</span><span class="n">solver_count</span><span class="p">);</span>
  <span class="n">Caffe</span><span class="o">::</span><span class="n">set_solver_rank</span><span class="p">(</span><span class="n">solver_rank</span><span class="p">);</span>
  <span class="n">Caffe</span><span class="o">::</span><span class="n">set_multiprocess</span><span class="p">(</span><span class="n">multiprocess</span><span class="p">);</span>

  <span class="n">InternalThreadEntry</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="datalayer">DataLayer</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">DataLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">BasePrefetchingDataLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="n">type</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="s">&#34;Data&#34;</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="n">ExactNumBottomBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="n">MinTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="n">MaxTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">2</span><span class="p">;</span> <span class="p">}</span>
    <span class="p">...</span>

<span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">DataLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">load_batch</span><span class="p">(</span><span class="n">Batch</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*</span> <span class="n">batch</span><span class="p">)</span> <span class="p">{</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Use data layer if the data is from a database, i.e., leveldb.
For example, refer to <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/examples/mnist/lenet_train_test.prototxt#L4">examples/mnist/lenet_train_test.prototxt</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="n">layer</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="n">name</span><span class="o">:</span> <span class="s">&#34;mnist&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">type</span><span class="o">:</span> <span class="s">&#34;Data&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">top</span><span class="o">:</span> <span class="s">&#34;data&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">top</span><span class="o">:</span> <span class="s">&#34;label&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">include</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">phase</span><span class="o">:</span> <span class="n">TRAIN</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="n">transform_param</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">scale</span><span class="o">:</span> <span class="mf">0.00390625</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="n">data_param</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">source</span><span class="o">:</span> <span class="s">&#34;examples/mnist/mnist_train_lmdb&#34;</span><span class="err">
</span><span class="err"></span>    <span class="n">batch_size</span><span class="o">:</span> <span class="mi">64</span><span class="err">
</span><span class="err"></span>    <span class="n">backend</span><span class="o">:</span> <span class="n">LMDB</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span><span class="p">}</span><span class="err">
</span><span class="err"></span><span class="n">layer</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="n">name</span><span class="o">:</span> <span class="s">&#34;mnist&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">type</span><span class="o">:</span> <span class="s">&#34;Data&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">top</span><span class="o">:</span> <span class="s">&#34;data&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">top</span><span class="o">:</span> <span class="s">&#34;label&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">include</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">phase</span><span class="o">:</span> <span class="n">TEST</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="n">transform_param</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">scale</span><span class="o">:</span> <span class="mf">0.00390625</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="n">data_param</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">source</span><span class="o">:</span> <span class="s">&#34;examples/mnist/mnist_test_lmdb&#34;</span><span class="err">
</span><span class="err"></span>    <span class="n">batch_size</span><span class="o">:</span> <span class="mi">100</span><span class="err">
</span><span class="err"></span>    <span class="n">backend</span><span class="o">:</span> <span class="n">LMDB</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="inputlayer">InputLayer</h2>

<p>See the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/input_layer.cpp#L8">code</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">InputLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">LayerSetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_top</span> <span class="o">=</span> <span class="n">top</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">InputParameter</span><span class="o">&amp;</span> <span class="n">param</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">input_param</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_shape</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">shape_size</span><span class="p">();</span>
  <span class="n">CHECK</span><span class="p">(</span><span class="n">num_shape</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">num_shape</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">num_shape</span> <span class="o">==</span> <span class="n">num_top</span><span class="p">)</span>
      <span class="o">&lt;&lt;</span> <span class="s">&#34;Must specify &#39;shape&#39; once, once per top blob, or not at all: &#34;</span>
      <span class="o">&lt;&lt;</span> <span class="n">num_top</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; tops vs. &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">num_shape</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; shapes.&#34;</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">num_shape</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_top</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">const</span> <span class="kt">int</span> <span class="n">shape_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="n">shape_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="n">i</span><span class="p">;</span>
      <span class="n">top</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">Reshape</span><span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">shape_index</span><span class="p">));</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// net.hpp
</span><span class="c1"></span>
  <span class="kr">inline</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">input_blobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">net_input_blobs_</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">BlobShape</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">repeated</span> <span class="kt">int64</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">[</span><span class="k">packed</span> <span class="o">=</span> <span class="kc">true</span><span class="p">];</span><span class="err">
</span><span class="err"></span><span class="p">}</span><span class="err">
</span><span class="err">
</span><span class="err"></span><span class="kd">message</span> <span class="nc">InputParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// This layer produces N &gt;= 1 top blob(s) to be assigned manually.
</span><span class="c1"></span>  <span class="c1">// Define N shapes to set a shape for each top.
</span><span class="c1"></span>  <span class="c1">// Define 1 shape to set the same shape for every top.
</span><span class="c1"></span>  <span class="c1">// Define no shape to defer to reshaping manually.
</span><span class="c1"></span>  <span class="k">repeated</span> <span class="n">BlobShape</span> <span class="n">shape</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="p">}</span><span class="err">
</span><span class="err">
</span><span class="err"></span><span class="n">name</span><span class="o">:</span> <span class="s">&#34;LeNet&#34;</span><span class="err">
</span><span class="err"></span><span class="n">layer</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="n">name</span><span class="o">:</span> <span class="s">&#34;data&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">type</span><span class="o">:</span> <span class="s">&#34;Input&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">top</span><span class="o">:</span> <span class="s">&#34;data&#34;</span><span class="err">
</span><span class="err"></span>  <span class="n">input_param</span> <span class="p">{</span> <span class="n">shape</span><span class="o">:</span> <span class="p">{</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">64</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">1</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">28</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">28</span> <span class="p">}</span> <span class="p">}</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>In <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/net.cpp#L98">net.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">    <span class="kt">int</span> <span class="n">num_top</span> <span class="o">=</span> <span class="n">layer_param</span><span class="p">.</span><span class="n">top_size</span><span class="p">();</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">top_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">top_id</span> <span class="o">&lt;</span> <span class="n">num_top</span><span class="p">;</span> <span class="o">++</span><span class="n">top_id</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">AppendTop</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">top_id</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">available_blobs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">blob_name_to_idx</span><span class="p">);</span>
      <span class="c1">// Collect Input layer tops as Net inputs.
</span><span class="c1"></span>      <span class="k">if</span> <span class="p">(</span><span class="n">layer_param</span><span class="p">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="s">&#34;Input&#34;</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">blob_id</span> <span class="o">=</span> <span class="n">blobs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">net_input_blob_indices_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">blob_id</span><span class="p">);</span>
        <span class="n">net_input_blobs_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">blobs_</span><span class="p">[</span><span class="n">blob_id</span><span class="p">].</span><span class="n">get</span><span class="p">());</span>
      <span class="p">}</span>
    <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>and in <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/examples/cpp_classification/classification.cpp#L176">examples/cpp_classification/classification.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="kt">void</span> <span class="n">Classifier</span><span class="o">::</span><span class="n">WrapInputLayer</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span><span class="o">&gt;*</span> <span class="n">input_channels</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Blob</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;*</span> <span class="n">input_layer</span> <span class="o">=</span> <span class="n">net_</span><span class="o">-&gt;</span><span class="n">input_blobs</span><span class="p">()[</span><span class="mi">0</span><span class="p">];</span>

  <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">-&gt;</span><span class="n">width</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">-&gt;</span><span class="n">height</span><span class="p">();</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">input_layer</span><span class="o">-&gt;</span><span class="n">channels</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">channel</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">CV_32FC1</span><span class="p">,</span> <span class="n">input_data</span><span class="p">);</span>
    <span class="n">input_channels</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="n">channel</span><span class="p">);</span>
    <span class="n">input_data</span> <span class="o">+=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">height</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Pay attention to <code>input_layer-&gt;mutable_cpu_data();</code>. It is
<code>mutable_cput_data()</code> instead of <code>cpu_data()</code> !!!</p>

<h2 id="neuron-layer">Neuron Layer</h2>

<p><a href="https://github.com/BVLC/caffe/blob/master/include/caffe/layers/neuron_layer.hpp">neuron_layer.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">NeuronLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Layer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
  <span class="k">explicit</span> <span class="n">NeuronLayer</span><span class="p">(</span><span class="k">const</span> <span class="n">LayerParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
     <span class="o">:</span> <span class="n">Layer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="p">{}</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">);</span>

  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">ExactNumBottomBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">ExactNumTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
<span class="p">};</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/neuron_layer.cpp">neuron_layer.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">NeuronLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ReshapeLike</span><span class="p">(</span><span class="o">*</span><span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="sigmod-layer">Sigmod Layer</h2>

<p>The sigmoid function is
$$
f(x) = \frac{1}{1+e^{-x}}
$$</p>

<p>The tanh function is
$$
g(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
= \frac{1 - e^{-2x}}{1+e^{-2x}}
= 1 - \frac{2e^{-2x}}{1+e^{-2x}}
$$</p>

<p>Note that
$$
\frac{1}{2} g(\frac{1}{2}x) + \frac{1}{2} =
\frac{1}{2} (1 - \frac{2e^{-x}}{1+e^{-x}}) + \frac{1}{2}
= 1 - \frac{e^{-x}}{1+e^{-x}}
= \frac{1}{1+e^{-x}}
$$</p>

<p>See the <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/sigmoid_layer.cpp#L8">code</a> below</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kr">inline</span> <span class="n">Dtype</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Dtype</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/sigmoid_layer.cpp#L14">forward pass</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SigmoidLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">top_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>The derivative of a sigmoid function is
$$
\frac{\mathrm{d}f}{\mathrm{d}x} =
\frac{e^{-x}}{(1+e^{-x})^2} =
\frac{1+e^{-x}-1}{(1+e^{-x})^2} =
\frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^2} =
= f - f^2
= f(1-f)
$$</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/sigmoid_layer.cpp#L25">backward</a> pass is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SigmoidLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_diff</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">();</span>
    <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">const</span> <span class="n">Dtype</span> <span class="n">sigmoid_x</span> <span class="o">=</span> <span class="n">top_data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
      <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_diff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">sigmoid_x</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">sigmoid_x</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="tanh-layer">tanh Layer</h2>

<p><a href="https://en.cppreference.com/w/cpp/numeric/math/tanh">std::tanh</a></p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/tanh_layer.cpp#L10">forward</a> pass</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">TanHLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">top_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>The derivative of tanh is
$$
\frac{\mathrm{d}g}{\mathrm{d}x} =
\frac{(e^x+e^{-x})(e^x+e^{-x}) - (e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}
= 1 - g^2
$$</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/tanh_layer.cpp#L22">backward</a> pass is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">TanHLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_diff</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">();</span>
    <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
    <span class="n">Dtype</span> <span class="n">tanhx</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">tanhx</span> <span class="o">=</span> <span class="n">top_data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
      <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_diff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tanhx</span> <span class="o">*</span> <span class="n">tanhx</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="relu-layer">ReLu Layer</h2>

<p>ReLU is short for Rectified Linear Unit.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="c1">// Message that stores parameters used by ReLULayer
</span><span class="c1"></span><span class="kd">message</span> <span class="nc">ReLUParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Allow non-zero slope for negative inputs to speed up optimization
</span><span class="c1"></span>  <span class="c1">// Described in:
</span><span class="c1"></span>  <span class="c1">// Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities
</span><span class="c1"></span>  <span class="c1">// improve neural network acoustic models. In ICML Workshop on Deep Learning
</span><span class="c1"></span>  <span class="c1">// for Audio, Speech, and Language Processing.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">float</span> <span class="n">negative_slope</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mi">0</span><span class="p">];</span><span class="err">
</span><span class="err"></span>  <span class="kd">enum</span> <span class="n">Engine</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">DEFAULT</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="n">CAFFE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="n">CUDNN</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="n">Engine</span> <span class="n">engine</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="n">DEFAULT</span><span class="p">];</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>Pay attention to <code>negative_slope</code>; it is called <strong>leaky</strong>
rectified linear unit. Its usage is shown below.
The paper in the comment is available <a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">here</a></p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/relu_layer.cpp#L9">forward</a> pass is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">ReLULayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">negative_slope</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">relu_param</span><span class="p">().</span><span class="n">negative_slope</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">top_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="o">+</span> <span class="n">negative_slope</span> <span class="o">*</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/relu_layer.cpp#L22">backward</a> pass is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">ReLULayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_diff</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">();</span>
    <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
    <span class="n">Dtype</span> <span class="n">negative_slope</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">relu_param</span><span class="p">().</span><span class="n">negative_slope</span><span class="p">();</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_diff</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">((</span><span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
          <span class="o">+</span> <span class="n">negative_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">));</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="prelu">PReLu</h2>

<p>PReLu is short for Parametric ReLu, which was proposed by
<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">He Kaiming</a> in 2015.</p>

<p>Note that in leaky ReLu, the negative slope is fixed while
in the PReLu, the negative slope is a learnable parameter.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">PReLUParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:
</span><span class="c1"></span>  <span class="c1">// Surpassing Human-Level Performance on ImageNet Classification, 2015.
</span><span class="c1"></span><span class="err">
</span><span class="err"></span>  <span class="c1">// Initial value of a_i. Default is a_i=0.25 for all i.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="n">FillerParameter</span> <span class="n">filler</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Whether or not slope parameters are shared across channels.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">bool</span> <span class="n">channel_shared</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="kc">false</span><span class="p">];</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="pooling-layer">Pooling Layer</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="kd">message</span> <span class="nc">PoolingParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="kd">enum</span> <span class="n">PoolMethod</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">MAX</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="n">AVE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="n">STOCHASTIC</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="n">PoolMethod</span> <span class="n">pool</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="n">MAX</span><span class="p">];</span> <span class="c1">// The pooling method
</span><span class="c1"></span><span class="o">...</span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="k">case</span> <span class="nl">PoolingParameter_PoolMethod_STOCHASTIC</span><span class="p">:</span>
    <span class="n">NOT_IMPLEMENTED</span><span class="p">;</span>
    <span class="k">break</span><span class="p">;</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="loss-layer">Loss Layer</h2>

<p>It takes two input:
    * index 0: predictions
    * index 1: ground truth</p>

<p><code>LossLayer</code> is the base class for other kinds of losses.</p>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/layers/loss_layer.hpp#L32">loss_layer.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">const</span> <span class="kt">float</span> <span class="n">kLOG_THRESHOLD</span> <span class="o">=</span> <span class="mf">1e-20</span><span class="p">;</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">LossLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Layer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="n">ExactNumBottomBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">2</span><span class="p">;</span> <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/loss_layer.cpp#L7">loss_layer.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">LayerSetUp</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// LossLayers have a non-zero (1) loss by default.
</span><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_weight_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">add_loss_weight</span><span class="p">(</span><span class="n">Dtype</span><span class="p">(</span><span class="mi">1</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Reshape</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
      <span class="o">&lt;&lt;</span> <span class="s">&#34;The data and label should have the same first dimension.&#34;</span><span class="p">;</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">loss_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>  <span class="c1">// Loss layers output a scalar; 0 axes.
</span><span class="c1"></span>  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">Reshape</span><span class="p">(</span><span class="n">loss_shape</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Note that it set <code>loss_weight[0]</code> to 1 and the output is a scalar.</p>

<h2 id="negative-log-loss-neg-loss">Negative Log Loss (neg loss)</h2>

<p>It is <code>MultinomialLogisticLossLayer</code> in Caffe.</p>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/layers/multinomial_logistic_loss_layer.hpp#L44">multinomial_logistic_loss_layer.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">MultinomialLogisticLossLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/multinomial_logistic_loss_layer.cpp#L11">multinomial_logistic_loss_layer.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">MultinomialLogisticLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Reshape</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Reshape</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
  <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">channels</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">height</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">width</span><span class="p">(),</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Note that <code>bottom[1]</code> is the ground truth and contains
a batch of scalars. <code>bottom[1]-&gt;num()</code> is the batch size.</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/multinomial_logistic_loss_layer.cpp#L20">forward</a> pass</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">MultinomialLogisticLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_label</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">num</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">();</span>
  <span class="n">Dtype</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">label</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">bottom_label</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">Dtype</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span>
        <span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">label</span><span class="p">],</span> <span class="n">Dtype</span><span class="p">(</span><span class="n">kLOG_THRESHOLD</span><span class="p">));</span>
    <span class="n">loss</span> <span class="o">-=</span> <span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">num</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>It first identifies the label of the input and then extracts
the corresponding prediction of the label and take the negative
log of the prediction as the loss. Since after the soft max operation
the prediction is in the range <code>[0,1]</code>, if the prediction is correct,
i.e., it is near 1, then the negative log of the prediction is nearly 0,
which is contributes less to the total loss; if the prediction is false,
i.e., it is near 0, then the negative log of the prediction is positively huge
and contributes a lot to the final loss.</p>

<p>The final loss is averaged over the batch size and is saved
into <code>top[0]</code>.</p>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/layer.hpp#L424">layer.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kr">inline</span> <span class="n">Dtype</span> <span class="n">Layer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Dtype</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">Reshape</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
  <span class="k">switch</span> <span class="p">(</span><span class="n">Caffe</span><span class="o">::</span><span class="n">mode</span><span class="p">())</span> <span class="p">{</span>
  <span class="k">case</span> <span class="n">Caffe</span><span class="o">::</span><span class="nl">CPU</span><span class="p">:</span>
    <span class="n">Forward_cpu</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">top_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">top_id</span> <span class="o">&lt;</span> <span class="n">top</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">top_id</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">loss</span><span class="p">(</span><span class="n">top_id</span><span class="p">))</span> <span class="p">{</span> <span class="k">continue</span><span class="p">;</span> <span class="p">}</span>
      <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="n">top_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">();</span>
      <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="n">top_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
      <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="n">top_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">();</span>
      <span class="n">loss</span> <span class="o">+=</span> <span class="n">caffe_cpu_dot</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">break</span><span class="p">;</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Note that <code>top[0]-&gt;cpu_diff()</code> is computed cumulatively above!
<code>this-&gt;loss(0)</code> has been initialized to 1 previously (in the base class <code>LossLayer</code>).</p>

<p><code>top[0]-&gt;cpu_data()</code> contains the loss averaged over one batch and <code>loss_weights[0]</code>
is 1, so <code>top[0]-&gt;cpu_diff()</code> contains the same value as
<code>top[0]-&gt;cpu_data</code>, which is used for backward propagation.</p>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/net.hpp#L85">net.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="n">Dtype</span> <span class="nf">ForwardBackward</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">Dtype</span> <span class="n">loss</span><span class="p">;</span>
    <span class="n">Forward</span><span class="p">(</span><span class="o">&amp;</span><span class="n">loss</span><span class="p">);</span>
    <span class="n">Backward</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">;</span>
  <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/net.cpp#L547">net.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward</span><span class="p">(</span><span class="n">Dtype</span><span class="o">*</span> <span class="n">loss</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">loss</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
    <span class="o">*</span><span class="n">loss</span> <span class="o">=</span> <span class="n">ForwardFromTo</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">layers_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">ForwardFromTo</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">layers_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">net_output_blobs_</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Dtype</span> <span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">ForwardFromTo</span><span class="p">(</span><span class="kt">int</span> <span class="n">start</span><span class="p">,</span> <span class="kt">int</span> <span class="n">end</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">CHECK_GE</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="n">CHECK_LT</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">layers_</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="n">Dtype</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">end</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">before_forward_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">before_forward_</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">run</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">Dtype</span> <span class="n">layer_loss</span> <span class="o">=</span> <span class="n">layers_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">Forward</span><span class="p">(</span><span class="n">bottom_vecs_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">top_vecs_</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="n">layer_loss</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">debug_info_</span><span class="p">)</span> <span class="p">{</span> <span class="n">ForwardDebugInfo</span><span class="p">(</span><span class="n">i</span><span class="p">);</span> <span class="p">}</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">after_forward_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">after_forward_</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">run</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">loss</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solver.cpp#L229">solver.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">    <span class="n">Dtype</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">param_</span><span class="p">.</span><span class="n">iter_size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">loss</span> <span class="o">+=</span> <span class="n">net_</span><span class="o">-&gt;</span><span class="n">ForwardBackward</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="n">loss</span> <span class="o">/=</span> <span class="n">param_</span><span class="p">.</span><span class="n">iter_size</span><span class="p">();</span>
</code></pre></td></tr></table>
</div>
</div>
<p>The above loss is averaged over iterations. Note that
is has already been averaged over batches.</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/multinomial_logistic_loss_layer.cpp#L37">backward</a> pass</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">MultinomialLogisticLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">()</span>
               <span class="o">&lt;&lt;</span> <span class="s">&#34; Layer cannot backpropagate to label inputs.&#34;</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_label</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
    <span class="kt">int</span> <span class="n">num</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">();</span>
    <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">();</span>
    <span class="n">caffe_set</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">bottom_diff</span><span class="p">);</span>
    <span class="k">const</span> <span class="n">Dtype</span> <span class="n">scale</span> <span class="o">=</span> <span class="o">-</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">num</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">label</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">bottom_label</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
      <span class="n">Dtype</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span>
          <span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">label</span><span class="p">],</span> <span class="n">Dtype</span><span class="p">(</span><span class="n">kLOG_THRESHOLD</span><span class="p">));</span>
      <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">/</span> <span class="n">prob</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Note that <code>top[0]-&gt;cpu_diff()</code> has been initialized in <code>layer.hpp</code> (see above).</p>

<p>Since <code>bottom[1]</code> is the ground truth label, it first checks that
<code>propagate_down[1]</code> is false.</p>

<p>$$
f = \frac{1}{n} \sum_{i}^{n}(-\log y_i)
$$</p>

<p>where f is the total loss.</p>

<p>$$
\frac{\partial f}{\partial y_i} =
-\frac{1}{n} \cdot \frac{1}{y_i}
$$</p>

<p>$\frac{\partial \mathrm{loss}}{\partial f}$ is equal to
<code>top[0]-&gt;cpu_diff()[0]</code></p>

<h2 id="softmax-layer">Softmax Layer</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="c1">// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer
</span><span class="c1"></span><span class="kd">message</span> <span class="nc">SoftmaxParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="kd">enum</span> <span class="n">Engine</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="n">DEFAULT</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="n">CAFFE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="n">CUDNN</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="k">optional</span> <span class="n">Engine</span> <span class="n">engine</span> <span class="o">=</span> <span class="mi">1</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="n">DEFAULT</span><span class="p">];</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// The axis along which to perform the softmax -- may be negative to index
</span><span class="c1"></span>  <span class="c1">// from the end (e.g., -1 for the last axis).
</span><span class="c1"></span>  <span class="c1">// Any other axes will be evaluated as independent softmaxes.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">int32</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mi">1</span><span class="p">];</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>Pay attention to <code>axis</code> above! Its default value is 1, which refers to
the number of channels.</p>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/layers/softmax_layer.hpp#L18">softmax_layer.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">SoftmaxLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Layer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="n">ExactNumBottomBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="n">ExactNumTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/softmax_layer.cpp">softmax_layer.hpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SoftmaxLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">softmax_axis_</span> <span class="o">=</span>
      <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">CanonicalAxisIndex</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">softmax_param</span><span class="p">().</span><span class="n">axis</span><span class="p">());</span>
  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ReshapeLike</span><span class="p">(</span><span class="o">*</span><span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">mult_dims</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="n">softmax_axis_</span><span class="p">));</span>
  <span class="n">sum_multiplier_</span><span class="p">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">mult_dims</span><span class="p">);</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">multiplier_data</span> <span class="o">=</span> <span class="n">sum_multiplier_</span><span class="p">.</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="n">caffe_set</span><span class="p">(</span><span class="n">sum_multiplier_</span><span class="p">.</span><span class="n">count</span><span class="p">(),</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">multiplier_data</span><span class="p">);</span>
  <span class="n">outer_num_</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">softmax_axis_</span><span class="p">);</span>
  <span class="n">inner_num_</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(</span><span class="n">softmax_axis_</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">scale_dims</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">();</span>
  <span class="n">scale_dims</span><span class="p">[</span><span class="n">softmax_axis_</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="n">scale_</span><span class="p">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">scale_dims</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>If the shape of the bottom is <code>2x3x4x5</code> and
<code>softmax_axis</code> is 1, then</p>

<ul>
<li><code>sum_multiplier_</code> is <code>[1, 1, 1]</code>, i.e., a vector of 3 scalars</li>
<li><code>outer_num_</code> is 2, which is the batch size</li>
<li><code>inner_num_</code> is 20, which is the product of 4 and 5</li>
<li><code>scale_</code> has the shape <code>2x1x4x5</code> (by fangjun: it should be 4x5)</li>
</ul>

<p>Every channel is a vector of 20 numbers and the output is
3 channel vectors, each of which is a  vector of 20 numbers.</p>

<p><code>out[0][0] = softmax(channel[0][0]; channel[0][0], channel[1][0], channel[2][0])</code></p>

<p><code>out[0][1] = softmax(channel[0][1]; channel[0][1], channel[1][1], channel[2][1])</code></p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/softmax_layer.cpp#L27">forward</a> pass</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SoftmaxLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_data</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">scale_data</span> <span class="o">=</span> <span class="n">scale_</span><span class="p">.</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="n">softmax_axis_</span><span class="p">);</span>
  <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">outer_num_</span><span class="p">;</span>
  <span class="n">caffe_copy</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">bottom_data</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
  <span class="c1">// We need to subtract the max to avoid numerical issues, compute the exp,
</span><span class="c1"></span>  <span class="c1">// and then normalize.
</span><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">outer_num_</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// initialize scale_data to the first plane
</span><span class="c1"></span>    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">inner_num_</span><span class="p">,</span> <span class="n">bottom_data</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">scale_data</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">channels</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">inner_num_</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">scale_data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">scale_data</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
            <span class="n">bottom_data</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">k</span><span class="p">]);</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">// subtraction
</span><span class="c1"></span>    <span class="n">caffe_cpu_gemm</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">inner_num_</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">sum_multiplier_</span><span class="p">.</span><span class="n">cpu_data</span><span class="p">(),</span> <span class="n">scale_data</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
    <span class="c1">// exponentiation
</span><span class="c1"></span>    <span class="n">caffe_exp</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">top_data</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
    <span class="c1">// sum after exp
</span><span class="c1"></span>    <span class="n">caffe_cpu_gemv</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CblasTrans</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">inner_num_</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">top_data</span><span class="p">,</span> <span class="n">sum_multiplier_</span><span class="p">.</span><span class="n">cpu_data</span><span class="p">(),</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">scale_data</span><span class="p">);</span>
    <span class="c1">// division
</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">channels</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">caffe_div</span><span class="p">(</span><span class="n">inner_num_</span><span class="p">,</span> <span class="n">top_data</span><span class="p">,</span> <span class="n">scale_data</span><span class="p">,</span> <span class="n">top_data</span><span class="p">);</span>
      <span class="n">top_data</span> <span class="o">+=</span> <span class="n">inner_num_</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>For the <code>subtraction</code>, <code>sum_multiplier</code> is of size
<code>channels x 1</code>, <code>scale_data</code> if of size <code>1 x inner_num_</code>,
<code>top_data</code> is of size <code>channels x inner_num_</code>. alpha is -1 and beta is 1.</p>

<p><code>sum_multiplier * scale_data</code> is to replicate the rows of max values
and then subtract it from the top data.</p>

<p>For the <code>exponentiation</code>, <code>top_data</code> is first transposed
to a shape <code>inner_num x channels</code>, then it is multiplied with
<code>sum_multiplier</code> (shape is <code>channels x 1</code>) and the results
are saved into <code>inner_num</code>.</p>

<p>For the <code>division</code>, the numerator is in <code>top_data</code> and the
denominator is in <code>scale_data</code></p>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/util/math_functions.cpp#L13">math_functions</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span><span class="o">&lt;&gt;</span>
<span class="kt">void</span> <span class="n">caffe_cpu_gemm</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="k">const</span> <span class="n">CBLAS_TRANSPOSE</span> <span class="n">TransA</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">CBLAS_TRANSPOSE</span> <span class="n">TransB</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">K</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">lda</span> <span class="o">=</span> <span class="p">(</span><span class="n">TransA</span> <span class="o">==</span> <span class="n">CblasNoTrans</span><span class="p">)</span> <span class="o">?</span> <span class="nl">K</span> <span class="p">:</span> <span class="n">M</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">ldb</span> <span class="o">=</span> <span class="p">(</span><span class="n">TransB</span> <span class="o">==</span> <span class="n">CblasNoTrans</span><span class="p">)</span> <span class="o">?</span> <span class="nl">N</span> <span class="p">:</span> <span class="n">K</span><span class="p">;</span>
  <span class="n">cblas_sgemm</span><span class="p">(</span><span class="n">CblasRowMajor</span><span class="p">,</span> <span class="n">TransA</span><span class="p">,</span> <span class="n">TransB</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">lda</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span>
      <span class="n">ldb</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;&gt;</span>
<span class="kt">void</span> <span class="n">caffe_cpu_gemv</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="k">const</span> <span class="n">CBLAS_TRANSPOSE</span> <span class="n">TransA</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cblas_sgemv</span><span class="p">(</span><span class="n">CblasRowMajor</span><span class="p">,</span> <span class="n">TransA</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://developer.apple.com/documentation/accelerate/1513264-cblas_sgemm?language=objc">cblas_sgemm</a> <code>C = alpha*A*B + beta*C</code>.
<code>A</code> is <code>MxK</code>, <code>B</code> is <code>KxN</code>, <code>C</code> is <code>MxN</code>.</p>

<p><a href="https://developer.apple.com/documentation/accelerate/1513065-cblas_sgemv?language=objc">cblas_sgemv</a>, <code>y = alpha * transpose(A) * x + beta * y</code>.
<code>A</code> is <code>MxN</code>.</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/softmax_layer.cpp#L63">backward</a> pass is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SoftmaxLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_diff</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">top_data</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
  <span class="n">Dtype</span><span class="o">*</span> <span class="n">scale_data</span> <span class="o">=</span> <span class="n">scale_</span><span class="p">.</span><span class="n">mutable_cpu_data</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="n">softmax_axis_</span><span class="p">);</span>
  <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">outer_num_</span><span class="p">;</span>
  <span class="n">caffe_copy</span><span class="p">(</span><span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">top_diff</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">outer_num_</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// compute dot(top_diff, top_data) and subtract them from the bottom diff
</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">inner_num_</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">scale_data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">caffe_cpu_strided_dot</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span>
          <span class="n">bottom_diff</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">inner_num_</span><span class="p">,</span>
          <span class="n">top_data</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">inner_num_</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// subtraction
</span><span class="c1"></span>    <span class="n">caffe_cpu_gemm</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">CblasNoTrans</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">inner_num_</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">sum_multiplier_</span><span class="p">.</span><span class="n">cpu_data</span><span class="p">(),</span> <span class="n">scale_data</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">bottom_diff</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">dim</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="c1">// elementwise multiplication
</span><span class="c1"></span>  <span class="n">caffe_mul</span><span class="p">(</span><span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">count</span><span class="p">(),</span> <span class="n">bottom_diff</span><span class="p">,</span> <span class="n">top_data</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Use the chain rule to show that the above is indeed correct.</p>

<p>The following shows the backward propagation rule of the softmax layer
in Caffe.</p>

<p>$$
f_i = \frac{e^{x_i}}{\sum_j e^j}
$$</p>

<p>$$
\frac{\partial f_k}{\partial x_i}
= \frac{[i==k]e^k \sum_j e^j - e^i e^k}{(\sum_j e^j)^2}
= e^k(\frac{[i==k]}{\sum_j e^j} - \frac{e^i}{(\sum_j e^j)^2})
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_i}
= \sum_k \frac{\partial \mathrm{loss}}{\partial f_k} \frac{\partial f_k}{x_i}
= \sum_k \frac{\partial \mathrm{loss}}{\partial f_k} e^k
(\frac{[i==k]}{\sum_j e^j} - \frac{e^i}{(\sum_j e^j)^2})
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_i}
= \sum_k \frac{\partial \mathrm{loss}}{\partial f_k}e^k\frac{[i==k]}{\sum_j e^j}
- \sum_k \frac{\partial \mathrm{loss}}{\partial f_k}\frac{e^k}{\sum_j e^j}\frac{e^i}{\sum_j e^j}
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_i}
= \frac{\partial \mathrm{loss}}{\partial f_i} f_i
- \sum_k \frac{\partial \mathrm{loss}}{\partial f_k} f_k f_i
= f_i (\frac{\partial \mathrm{loss}}{\partial f_i} - \sum_k \frac{\partial \mathrm{loss}}{\partial f_k} f_k)
$$</p>

<p>In the code, <code>bottom_diff</code> is $\frac{\partial \mathrm{loss}}{\partial f_k}$, which is copied from <code>top_diff</code>.</p>

<p><code>compute dot(top_diff, top_data)</code> computes the inner product in the above euqation:
$$
\sum_k \frac{\partial \mathrm{loss}}{\partial f_k} f_k
$$</p>

<p><code>subtraction</code> implements the expression in the
parenthesis in the above equation:
$$
\frac{\partial \mathrm{loss}}{\partial f_i} - \sum_k \frac{\partial \mathrm{loss}}{\partial f_k} f_k
$$</p>

<p><code>elementwise multiplication</code> is the last step in the above equation:
$$
f_i (\frac{\partial \mathrm{loss}}{\partial f_i} - \sum_k \frac{\partial \mathrm{loss}}{\partial f_k} f_k)
$$</p>

<h2 id="softmax-with-loss-layer">SoftMax with Loss Layer</h2>

<p>Loss here means negative log loss. The comment in the source code
called it as cross entropy loss. It is also known as
multinomial logistic loss.</p>

<p>Note that there is no cross entropy loss layer in caffe.
Caffe provides <code>MultinomialLogisticLayer</code>.</p>

<p><code>SoftmaxWithLossLayer</code> combines <code>SoftMaxLayer</code>
and <code>MultinomialLogisticLossLayer</code>. The gradient
propagation is much simpler than
<code>SoftMaxLayer</code> and
<code>MultinomialLogisticLossLayer</code>.</p>

<p>Note that we cannot ignore labels in
<code>MultinomialLogisticLossLayer</code>
but we can ignore labels in <code>SoftmaxWithLossLayer</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="k">class</span><span class="err"> </span><span class="nc">SoftmaxWithLossLayer</span> <span class="o">:</span> <span class="k">public</span> <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="k">public</span><span class="o">:</span>
   <span class="cm">/**
</span><span class="cm">    * @param param provides LossParameter loss_param, with options:
</span><span class="cm">    *  - ignore_label (optional)
</span><span class="cm">    *    Specify a label value that should be ignored when computing the loss.
</span><span class="cm">    *  - normalize (optional, default true)
</span><span class="cm">    *    If true, the loss is normalized by the number of (nonignored) labels
</span><span class="cm">    *    present; otherwise the loss is simply summed over spatial locations.
</span><span class="cm">    */</span>
  <span class="k">explicit</span> <span class="n">SoftmaxWithLossLayer</span><span class="p">(</span><span class="k">const</span> <span class="n">LayerParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="p">{}</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="n">LayerSetUp</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">);</span>
  <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">Reshape</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span>
      <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">);</span>

  <span class="k">virtual</span> <span class="kr">inline</span> <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="nf">type</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="s">&#34;SoftmaxWithLoss&#34;</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">ExactNumTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">MinTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
  <span class="k">virtual</span> <span class="kr">inline</span> <span class="kt">int</span> <span class="nf">MaxTopBlobs</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">2</span><span class="p">;</span> <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>If it has two tops, then <code>top[1]</code> is the output of the softmax.
<code>top[0]</code> is always the loss.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-proto" data-lang="proto"><span class="c1">// Message that stores parameters shared by loss layers
</span><span class="c1"></span><span class="kd">message</span> <span class="nc">LossParameter</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// If specified, ignore instances with the given label.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">int32</span> <span class="n">ignore_label</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="c1">// How to normalize the loss for loss layers that aggregate across batches,
</span><span class="c1"></span>  <span class="c1">// spatial dimensions, or other dimensions.  Currently only implemented in
</span><span class="c1"></span>  <span class="c1">// SoftmaxWithLoss and SigmoidCrossEntropyLoss layers.
</span><span class="c1"></span>  <span class="kd">enum</span> <span class="n">NormalizationMode</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>    <span class="c1">// Divide by the number of examples in the batch times spatial dimensions.
</span><span class="c1"></span>    <span class="c1">// Outputs that receive the ignore label will NOT be ignored in computing
</span><span class="c1"></span>    <span class="c1">// the normalization factor.
</span><span class="c1"></span>    <span class="n">FULL</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="c1">// Divide by the total number of output locations that do not take the
</span><span class="c1"></span>    <span class="c1">// ignore_label.  If ignore_label is not set, this behaves like FULL.
</span><span class="c1"></span>    <span class="n">VALID</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="c1">// Divide by the batch size.
</span><span class="c1"></span>    <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span>    <span class="c1">// Do not normalize the loss.
</span><span class="c1"></span>    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span><span class="err">
</span><span class="err"></span>  <span class="p">}</span><span class="err">
</span><span class="err"></span>  <span class="c1">// For historical reasons, the default normalization for
</span><span class="c1"></span>  <span class="c1">// SigmoidCrossEntropyLoss is BATCH_SIZE and *not* VALID.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="n">NormalizationMode</span> <span class="n">normalization</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="n">VALID</span><span class="p">];</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Deprecated.  Ignored if normalization is specified.  If normalization
</span><span class="c1"></span>  <span class="c1">// is not specified, then setting this to false will be equivalent to
</span><span class="c1"></span>  <span class="c1">// normalization = BATCH_SIZE to be consistent with previous behavior.
</span><span class="c1"></span>  <span class="k">optional</span> <span class="kt">bool</span> <span class="n">normalize</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span><span class="err">
</span><span class="err"></span><span class="p">}</span></code></pre></td></tr></table>
</div>
</div>
<p>It is possible to ignore one class via <code>ignore_label</code>.
It is impossible to ignore more than one class without changing
the caffe source code.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SoftmaxWithLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">LayerSetUp</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">LossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">LayerSetUp</span><span class="p">(</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">);</span>
  <span class="n">LayerParameter</span> <span class="nf">softmax_param</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">);</span>
  <span class="n">softmax_param</span><span class="p">.</span><span class="n">set_type</span><span class="p">(</span><span class="s">&#34;Softmax&#34;</span><span class="p">);</span>
  <span class="n">softmax_layer_</span> <span class="o">=</span> <span class="n">LayerRegistry</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">CreateLayer</span><span class="p">(</span><span class="n">softmax_param</span><span class="p">);</span>
  <span class="n">softmax_bottom_vec_</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
  <span class="n">softmax_bottom_vec_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
  <span class="n">softmax_top_vec_</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
  <span class="n">softmax_top_vec_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prob_</span><span class="p">);</span>
  <span class="n">softmax_layer_</span><span class="o">-&gt;</span><span class="n">SetUp</span><span class="p">(</span><span class="n">softmax_bottom_vec_</span><span class="p">,</span> <span class="n">softmax_top_vec_</span><span class="p">);</span>

  <span class="n">has_ignore_label_</span> <span class="o">=</span>
    <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_param</span><span class="p">().</span><span class="n">has_ignore_label</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">has_ignore_label_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">ignore_label_</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_param</span><span class="p">().</span><span class="n">ignore_label</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_param</span><span class="p">().</span><span class="n">has_normalization</span><span class="p">()</span> <span class="o">&amp;&amp;</span>
      <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_param</span><span class="p">().</span><span class="n">has_normalize</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">normalization_</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_param</span><span class="p">().</span><span class="n">normalize</span><span class="p">()</span> <span class="o">?</span>
                     <span class="nl">LossParameter_NormalizationMode_VALID</span> <span class="p">:</span>
                     <span class="n">LossParameter_NormalizationMode_BATCH_SIZE</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">normalization_</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">layer_param_</span><span class="p">.</span><span class="n">loss_param</span><span class="p">().</span><span class="n">normalization</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Note that if <code>has_ignore_label</code> is set, <code>ignore_label_</code> saves
the ignored label. Labels must be continuous and counted from 0.</p>

<p>It uses <code>softmax_layer_</code> internally. The above code also sets up
the bottom and top vectors of the softmax layer.</p>

<p><code>softmax_bottom_vec_</code> saves <code>bottom[0]</code>, which is the data.
<code>bottom[1]</code> saves labels.</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/softmax_loss_layer.cpp#L89">forward</a> pass</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SoftmaxWithLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Forward_cpu</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// The forward pass computes the softmax prob values.
</span><span class="c1"></span>  <span class="n">softmax_layer_</span><span class="o">-&gt;</span><span class="n">Forward</span><span class="p">(</span><span class="n">softmax_bottom_vec_</span><span class="p">,</span> <span class="n">softmax_top_vec_</span><span class="p">);</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">prob_data</span> <span class="o">=</span> <span class="n">prob_</span><span class="p">.</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">label</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">prob_</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">outer_num_</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">Dtype</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">outer_num_</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">inner_num_</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">const</span> <span class="kt">int</span> <span class="n">label_value</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">has_ignore_label_</span> <span class="o">&amp;&amp;</span> <span class="n">label_value</span> <span class="o">==</span> <span class="n">ignore_label_</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">continue</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="n">DCHECK_GE</span><span class="p">(</span><span class="n">label_value</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
      <span class="n">DCHECK_LT</span><span class="p">(</span><span class="n">label_value</span><span class="p">,</span> <span class="n">prob_</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">softmax_axis_</span><span class="p">));</span>
      <span class="n">loss</span> <span class="o">-=</span> <span class="n">log</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">prob_data</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">label_value</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">],</span>
                           <span class="n">Dtype</span><span class="p">(</span><span class="n">FLT_MIN</span><span class="p">)));</span>
      <span class="o">++</span><span class="n">count</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_data</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">get_normalizer</span><span class="p">(</span><span class="n">normalization_</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">top</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">top</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ShareData</span><span class="p">(</span><span class="n">prob_</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>If the output shape is <code>4x5x6x7</code>, it means the batch size is 4, which is
<code>outer_num_</code>; number of classes is <code>5</code>. The image size is <code>6x7</code> and each
pixel is classified into class 0,1,2,3 or 4. The output is a probability
map image with 5 channels.</p>

<p>Note the following code in the above</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">      <span class="k">if</span> <span class="p">(</span><span class="n">has_ignore_label_</span> <span class="o">&amp;&amp;</span> <span class="n">label_value</span> <span class="o">==</span> <span class="n">ignore_label_</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">continue</span><span class="p">;</span>
      <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">Dtype</span> <span class="n">SoftmaxWithLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">get_normalizer</span><span class="p">(</span>
    <span class="n">LossParameter_NormalizationMode</span> <span class="n">normalization_mode</span><span class="p">,</span> <span class="kt">int</span> <span class="n">valid_count</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">Dtype</span> <span class="n">normalizer</span><span class="p">;</span>
  <span class="k">switch</span> <span class="p">(</span><span class="n">normalization_mode</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">case</span> <span class="nl">LossParameter_NormalizationMode_FULL</span><span class="p">:</span>
      <span class="n">normalizer</span> <span class="o">=</span> <span class="n">Dtype</span><span class="p">(</span><span class="n">outer_num_</span> <span class="o">*</span> <span class="n">inner_num_</span><span class="p">);</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="k">case</span> <span class="nl">LossParameter_NormalizationMode_VALID</span><span class="p">:</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">valid_count</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">normalizer</span> <span class="o">=</span> <span class="n">Dtype</span><span class="p">(</span><span class="n">outer_num_</span> <span class="o">*</span> <span class="n">inner_num_</span><span class="p">);</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">normalizer</span> <span class="o">=</span> <span class="n">Dtype</span><span class="p">(</span><span class="n">valid_count</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="k">case</span> <span class="nl">LossParameter_NormalizationMode_BATCH_SIZE</span><span class="p">:</span>
      <span class="n">normalizer</span> <span class="o">=</span> <span class="n">Dtype</span><span class="p">(</span><span class="n">outer_num_</span><span class="p">);</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="k">case</span> <span class="nl">LossParameter_NormalizationMode_NONE</span><span class="p">:</span>
      <span class="n">normalizer</span> <span class="o">=</span> <span class="n">Dtype</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="k">default</span><span class="o">:</span>
      <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Unknown normalization mode: &#34;</span>
          <span class="o">&lt;&lt;</span> <span class="n">LossParameter_NormalizationMode_Name</span><span class="p">(</span><span class="n">normalization_mode</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="c1">// Some users will have no labels for some examples in order to &#39;turn off&#39; a
</span><span class="c1"></span>  <span class="c1">// particular loss in a multi-task setup. The max prevents NaNs in that case.
</span><span class="c1"></span>  <span class="k">return</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">Dtype</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">normalizer</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>If the ignore label is not set, then
<code>LossParameter_NormalizationMode_FULL</code> is the same as
<code>LossParameter_NormalizationMode_VALID</code>.</p>

<p>If the ignore label is set,
<code>LossParameter_NormalizationMode_FULL</code> is <code>outer_number * inner_number</code>
and
<code>LossParameter_NormalizationMode_VALID</code> is number of labels that contributes
to the final loss.</p>

<p>The <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/layers/softmax_loss_layer.cpp#L118">backward</a> pass is</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SoftmaxWithLossLayer</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Backward_cpu</span><span class="p">(</span><span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">top</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;&amp;</span> <span class="n">propagate_down</span><span class="p">,</span> <span class="k">const</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;*&gt;&amp;</span> <span class="n">bottom</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">()</span>
               <span class="o">&lt;&lt;</span> <span class="s">&#34; Layer cannot backpropagate to label inputs.&#34;</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">propagate_down</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">{</span>
    <span class="n">Dtype</span><span class="o">*</span> <span class="n">bottom_diff</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">mutable_cpu_diff</span><span class="p">();</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">prob_data</span> <span class="o">=</span> <span class="n">prob_</span><span class="p">.</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="n">caffe_copy</span><span class="p">(</span><span class="n">prob_</span><span class="p">.</span><span class="n">count</span><span class="p">(),</span> <span class="n">prob_data</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
    <span class="k">const</span> <span class="n">Dtype</span><span class="o">*</span> <span class="n">label</span> <span class="o">=</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_data</span><span class="p">();</span>
    <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">prob_</span><span class="p">.</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="n">outer_num_</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">outer_num_</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">inner_num_</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">const</span> <span class="kt">int</span> <span class="n">label_value</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">has_ignore_label_</span> <span class="o">&amp;&amp;</span> <span class="n">label_value</span> <span class="o">==</span> <span class="n">ignore_label_</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="n">softmax_axis_</span><span class="p">);</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
          <span class="p">}</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
          <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">label_value</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span><span class="p">;</span>
          <span class="o">++</span><span class="n">count</span><span class="p">;</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">// Scale gradient
</span><span class="c1"></span>    <span class="n">Dtype</span> <span class="n">loss_weight</span> <span class="o">=</span> <span class="n">top</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">cpu_diff</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span>
                        <span class="n">get_normalizer</span><span class="p">(</span><span class="n">normalization_</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
    <span class="n">caffe_scal</span><span class="p">(</span><span class="n">prob_</span><span class="p">.</span><span class="n">count</span><span class="p">(),</span> <span class="n">loss_weight</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Note that the backward gradient propagation is much simpler than
the softmax layer!</p>

<p>Suppose the input feature has 3 components: $x_0$, $x_1$ and $x_2$.
After the softmax layer, we have
$$
y_0 = \frac{e^{x_0}}{e^{x_0} + e^{x_1} + e^{x_2}}
$$</p>

<p>$$
y_1 = \frac{e^{x_1}}{e^{x_0} + e^{x_1} + e^{x_2}}
$$</p>

<p>$$
y_2 = \frac{e^{x_2}}{e^{x_0} + e^{x_1} + e^{x_2}}
$$</p>

<p>If the label is 1, that is $y_1$ is the predicated output, the loss is</p>

<p>$$
\mathrm{loss} = -\log y_1
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial y_0} = 0
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial y_1} = - \frac{1}{y_1}
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial y_2} = 0
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_0}
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_0} =
\frac{\partial \mathrm{loss}}{\partial y_0}
\frac{\partial y_0}{\partial x_0}
+
\frac{\partial \mathrm{loss}}{\partial y_1}
\frac{\partial y_1}{\partial x_0}
+
\frac{\partial \mathrm{loss}}{\partial y_2}
\frac{\partial y_2}{\partial x_0} =
-\frac{1}{y_1} \cdot (-y_0 y_1) = y_0
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_1}
= \frac{\partial \mathrm{loss}}{\partial y_1}
\frac{\partial y_1}{\partial x_1}
= -\frac{1}{y_1}\cdot y_1(1-y_1) = -(1 - y_1)
= y_1 - 1
$$</p>

<p>$$
\frac{\partial \mathrm{loss}}{\partial x_2}
= y_2
$$</p>

<p>The line</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="n">caffe_copy</span><span class="p">(</span><span class="n">prob_</span><span class="p">.</span><span class="n">count</span><span class="p">(),</span> <span class="n">prob_data</span><span class="p">,</span> <span class="n">bottom_diff</span><span class="p">);</span>
</code></pre></td></tr></table>
</div>
</div>
<p>first copies <code>y_1</code>, <code>y_2</code> and <code>y_3</code> to the gradient which are computed as above.</p>

<p>The line</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">label_value</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span><span class="p">;</span>
</code></pre></td></tr></table>
</div>
</div>
<p>computes <code>y_1 - 1</code>.</p>

<p>If the label is ignored, then there is no gradient for
<code>x_1</code>, <code>x_2</code> and <code>x_3</code>, which is accomplished by the line</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">        <span class="k">const</span> <span class="kt">int</span> <span class="n">label_value</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">has_ignore_label_</span> <span class="o">&amp;&amp;</span> <span class="n">label_value</span> <span class="o">==</span> <span class="n">ignore_label_</span><span class="p">)</span> <span class="p">{</span>
          <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">bottom</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">(</span><span class="n">softmax_axis_</span><span class="p">);</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">bottom_diff</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">inner_num_</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
          <span class="p">}</span>
        <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<h2 id="differences-between-snapshot-and-weights">Differences Between Snapshot and Weights</h2>

<h3 id="snapshot">Snapshot</h3>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solver.cpp#L420">sovler.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Snapshot</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">CHECK</span><span class="p">(</span><span class="n">Caffe</span><span class="o">::</span><span class="n">root_solver</span><span class="p">());</span>
  <span class="n">string</span> <span class="n">model_filename</span><span class="p">;</span>
  <span class="k">switch</span> <span class="p">(</span><span class="n">param_</span><span class="p">.</span><span class="n">snapshot_format</span><span class="p">())</span> <span class="p">{</span>
  <span class="k">case</span> <span class="n">caffe</span><span class="o">::</span><span class="nl">SolverParameter_SnapshotFormat_BINARYPROTO</span><span class="p">:</span>
    <span class="n">model_filename</span> <span class="o">=</span> <span class="n">SnapshotToBinaryProto</span><span class="p">();</span>
    <span class="k">break</span><span class="p">;</span>
  <span class="k">case</span> <span class="n">caffe</span><span class="o">::</span><span class="nl">SolverParameter_SnapshotFormat_HDF5</span><span class="p">:</span>
    <span class="n">model_filename</span> <span class="o">=</span> <span class="n">SnapshotToHDF5</span><span class="p">();</span>
    <span class="k">break</span><span class="p">;</span>
  <span class="k">default</span><span class="o">:</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Unsupported snapshot format.&#34;</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">SnapshotSolverState</span><span class="p">(</span><span class="n">model_filename</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="n">string</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">SnapshotToBinaryProto</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">string</span> <span class="n">model_filename</span> <span class="o">=</span> <span class="n">SnapshotFilename</span><span class="p">(</span><span class="s">&#34;.caffemodel&#34;</span><span class="p">);</span>
  <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Snapshotting to binary proto file &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">model_filename</span><span class="p">;</span>
  <span class="n">NetParameter</span> <span class="n">net_param</span><span class="p">;</span>
  <span class="n">net_</span><span class="o">-&gt;</span><span class="n">ToProto</span><span class="p">(</span><span class="o">&amp;</span><span class="n">net_param</span><span class="p">,</span> <span class="n">param_</span><span class="p">.</span><span class="n">snapshot_diff</span><span class="p">());</span>
  <span class="n">WriteProtoToBinaryFile</span><span class="p">(</span><span class="n">net_param</span><span class="p">,</span> <span class="n">model_filename</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">model_filename</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L257">sgd_solver.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">SnapshotSolverState</span><span class="p">(</span><span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">model_filename</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">switch</span> <span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">param_</span><span class="p">.</span><span class="n">snapshot_format</span><span class="p">())</span> <span class="p">{</span>
    <span class="k">case</span> <span class="n">caffe</span><span class="o">::</span><span class="nl">SolverParameter_SnapshotFormat_BINARYPROTO</span><span class="p">:</span>
      <span class="n">SnapshotSolverStateToBinaryProto</span><span class="p">(</span><span class="n">model_filename</span><span class="p">);</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="k">case</span> <span class="n">caffe</span><span class="o">::</span><span class="nl">SolverParameter_SnapshotFormat_HDF5</span><span class="p">:</span>
      <span class="n">SnapshotSolverStateToHDF5</span><span class="p">(</span><span class="n">model_filename</span><span class="p">);</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="k">default</span><span class="o">:</span>
      <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Unsupported snapshot format.&#34;</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">SnapshotSolverStateToBinaryProto</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">model_filename</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">SolverState</span> <span class="n">state</span><span class="p">;</span>
  <span class="n">state</span><span class="p">.</span><span class="n">set_iter</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">iter_</span><span class="p">);</span>
  <span class="n">state</span><span class="p">.</span><span class="n">set_learned_net</span><span class="p">(</span><span class="n">model_filename</span><span class="p">);</span>
  <span class="n">state</span><span class="p">.</span><span class="n">set_current_step</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">current_step_</span><span class="p">);</span>
  <span class="n">state</span><span class="p">.</span><span class="n">clear_history</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">history_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Add history
</span><span class="c1"></span>    <span class="n">BlobProto</span><span class="o">*</span> <span class="n">history_blob</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">add_history</span><span class="p">();</span>
    <span class="n">history_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ToProto</span><span class="p">(</span><span class="n">history_blob</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">string</span> <span class="n">snapshot_filename</span> <span class="o">=</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">SnapshotFilename</span><span class="p">(</span><span class="s">&#34;.solverstate&#34;</span><span class="p">);</span>
  <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span>
    <span class="o">&lt;&lt;</span> <span class="s">&#34;Snapshotting solver state to binary proto file &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">snapshot_filename</span><span class="p">;</span>
  <span class="n">WriteProtoToBinaryFile</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">snapshot_filename</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L222">tools/caffe.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="k">if</span> <span class="p">(</span><span class="n">FLAGS_snapshot</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">solver_param</span><span class="p">.</span><span class="n">clear_weights</span><span class="p">();</span>
  <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">FLAGS_weights</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">solver_param</span><span class="p">.</span><span class="n">clear_weights</span><span class="p">();</span>
    <span class="n">solver_param</span><span class="p">.</span><span class="n">add_weights</span><span class="p">(</span><span class="n">FLAGS_weights</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">FLAGS_snapshot</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Resuming from &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">FLAGS_snapshot</span><span class="p">;</span>
    <span class="n">solver</span><span class="o">-&gt;</span><span class="n">Restore</span><span class="p">(</span><span class="n">FLAGS_snapshot</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
  <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solver.cpp#L480">solver.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">Solver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">Restore</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="n">state_file</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">string</span> <span class="n">state_filename</span><span class="p">(</span><span class="n">state_file</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">state_filename</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">3</span> <span class="o">&amp;&amp;</span>
      <span class="n">state_filename</span><span class="p">.</span><span class="n">compare</span><span class="p">(</span><span class="n">state_filename</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s">&#34;.h5&#34;</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">RestoreSolverStateFromHDF5</span><span class="p">(</span><span class="n">state_filename</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">RestoreSolverStateFromBinaryProto</span><span class="p">(</span><span class="n">state_filename</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solvers/sgd_solver.cpp#L323">sgd_solver.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">SGDSolver</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">RestoreSolverStateFromBinaryProto</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">state_file</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">SolverState</span> <span class="n">state</span><span class="p">;</span>
  <span class="n">ReadProtoFromBinaryFile</span><span class="p">(</span><span class="n">state_file</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="p">);</span>
  <span class="k">this</span><span class="o">-&gt;</span><span class="n">iter_</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">iter</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">has_learned_net</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">NetParameter</span> <span class="n">net_param</span><span class="p">;</span>
    <span class="n">ReadNetParamsFromBinaryFileOrDie</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">learned_net</span><span class="p">().</span><span class="n">c_str</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">net_param</span><span class="p">);</span>
    <span class="k">this</span><span class="o">-&gt;</span><span class="n">net_</span><span class="o">-&gt;</span><span class="n">CopyTrainedLayersFrom</span><span class="p">(</span><span class="n">net_param</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">this</span><span class="o">-&gt;</span><span class="n">current_step_</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">current_step</span><span class="p">();</span>
  <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">history_size</span><span class="p">(),</span> <span class="n">history_</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
      <span class="o">&lt;&lt;</span> <span class="s">&#34;Incorrect length of history blobs.&#34;</span><span class="p">;</span>
  <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;SGDSolver: restoring history&#34;</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">history_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">history_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">FromProto</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">history</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/net.cpp#L735">net.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;::</span><span class="n">CopyTrainedLayersFrom</span><span class="p">(</span><span class="k">const</span> <span class="n">NetParameter</span><span class="o">&amp;</span> <span class="n">param</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">num_source_layers</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">layer_size</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_source_layers</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">LayerParameter</span><span class="o">&amp;</span> <span class="n">source_layer</span> <span class="o">=</span> <span class="n">param</span><span class="p">.</span><span class="n">layer</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
    <span class="k">const</span> <span class="n">string</span><span class="o">&amp;</span> <span class="n">source_layer_name</span> <span class="o">=</span> <span class="n">source_layer</span><span class="p">.</span><span class="n">name</span><span class="p">();</span>
    <span class="kt">int</span> <span class="n">target_layer_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">target_layer_id</span> <span class="o">!=</span> <span class="n">layer_names_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&amp;&amp;</span>
        <span class="n">layer_names_</span><span class="p">[</span><span class="n">target_layer_id</span><span class="p">]</span> <span class="o">!=</span> <span class="n">source_layer_name</span><span class="p">)</span> <span class="p">{</span>
      <span class="o">++</span><span class="n">target_layer_id</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">target_layer_id</span> <span class="o">==</span> <span class="n">layer_names_</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Ignoring source layer &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">source_layer_name</span><span class="p">;</span>
      <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">DLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Copying source layer &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">source_layer_name</span><span class="p">;</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="o">&gt;&amp;</span> <span class="n">target_blobs</span> <span class="o">=</span>
        <span class="n">layers_</span><span class="p">[</span><span class="n">target_layer_id</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">blobs</span><span class="p">();</span>
    <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">target_blobs</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">source_layer</span><span class="p">.</span><span class="n">blobs_size</span><span class="p">())</span>
        <span class="o">&lt;&lt;</span> <span class="s">&#34;Incompatible number of blobs for layer &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">source_layer_name</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">target_blobs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">target_blobs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">ShapeEquals</span><span class="p">(</span><span class="n">source_layer</span><span class="p">.</span><span class="n">blobs</span><span class="p">(</span><span class="n">j</span><span class="p">)))</span> <span class="p">{</span>
        <span class="n">Blob</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="n">source_blob</span><span class="p">;</span>
        <span class="k">const</span> <span class="kt">bool</span> <span class="n">kReshape</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
        <span class="n">source_blob</span><span class="p">.</span><span class="n">FromProto</span><span class="p">(</span><span class="n">source_layer</span><span class="p">.</span><span class="n">blobs</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="n">kReshape</span><span class="p">);</span>
        <span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Cannot copy param &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">j</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; weights from layer &#39;&#34;</span>
            <span class="o">&lt;&lt;</span> <span class="n">source_layer_name</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;&#39;; shape mismatch.  Source param shape is &#34;</span>
            <span class="o">&lt;&lt;</span> <span class="n">source_blob</span><span class="p">.</span><span class="n">shape_string</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;; target param shape is &#34;</span>
            <span class="o">&lt;&lt;</span> <span class="n">target_blobs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">shape_string</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;. &#34;</span>
            <span class="o">&lt;&lt;</span> <span class="s">&#34;To learn this layer&#39;s parameters from scratch rather than &#34;</span>
            <span class="o">&lt;&lt;</span> <span class="s">&#34;copying from a saved net, rename the layer.&#34;</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="k">const</span> <span class="kt">bool</span> <span class="n">kReshape</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
      <span class="n">target_blobs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">FromProto</span><span class="p">(</span><span class="n">source_layer</span><span class="p">.</span><span class="n">blobs</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="n">kReshape</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Conclusion: snapshots save not only the weights of the network but also
the states of the solver. They are saved to two files, one for the network
weights and the other one for the solver state. For the sgd solver, the solver
state contains the current iteration number and the current step (for computing learning rate), and the weight gradient for momentum update (i.e., the history vector above).</p>

<p>It is useful for continuing the training from the last time point
where it was stopped for some reason. If the solver state is not
available and we only have the network weights at present, then it
is called fine tuning.</p>

<h3 id="weights">Weights</h3>

<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/tools/caffe.cpp#L168">tools/caffe.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="n">CHECK</span><span class="p">(</span><span class="o">!</span><span class="n">FLAGS_snapshot</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">||</span> <span class="o">!</span><span class="n">FLAGS_weights</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
      <span class="o">&lt;&lt;</span> <span class="s">&#34;Give a snapshot to resume training or weights to finetune &#34;</span>
      <span class="s">&#34;but not both.&#34;</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">FLAGS_snapshot</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">solver_param</span><span class="p">.</span><span class="n">clear_weights</span><span class="p">();</span>
  <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">FLAGS_weights</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">solver_param</span><span class="p">.</span><span class="n">clear_weights</span><span class="p">();</span>
    <span class="n">solver_param</span><span class="p">.</span><span class="n">add_weights</span><span class="p">(</span><span class="n">FLAGS_weights</span><span class="p">);</span>
  <span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p><a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/solver.cpp#L116">solver.cpp</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-cpp" data-lang="cpp">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">w_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">w_idx</span> <span class="o">&lt;</span> <span class="n">param_</span><span class="p">.</span><span class="n">weights_size</span><span class="p">();</span> <span class="o">++</span><span class="n">w_idx</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">LoadNetWeights</span><span class="p">(</span><span class="n">net_</span><span class="p">,</span> <span class="n">param_</span><span class="p">.</span><span class="n">weights</span><span class="p">(</span><span class="n">w_idx</span><span class="p">));</span>
  <span class="p">}</span>

<span class="c1">// Load weights from the caffemodel(s) specified in &#34;weights&#34; solver parameter
</span><span class="c1">// into the train and test nets.
</span><span class="c1"></span><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">Dtype</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">LoadNetWeights</span><span class="p">(</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Net</span><span class="o">&lt;</span><span class="n">Dtype</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">net</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">model_list</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">model_names</span><span class="p">;</span>
  <span class="n">boost</span><span class="o">::</span><span class="n">split</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">model_list</span><span class="p">,</span> <span class="n">boost</span><span class="o">::</span><span class="n">is_any_of</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">model_names</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">boost</span><span class="o">::</span><span class="n">trim</span><span class="p">(</span><span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">LOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Finetuning from &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">net</span><span class="o">-&gt;</span><span class="n">CopyTrainedLayersFrom</span><span class="p">(</span><span class="n">model_names</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div>
<p>Conclusion: weights contain only the network weight and the solver state
is missing. It is useful for fine tuning.
Refer to <a href="http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html">Fine-tuning CaffeNet for Style Recognition on Flickr Style Data</a>.</p>

<h2 id="convolution">Convolution</h2>

<p>It is actually correlation from the perspective of the image processing!
In addition, there is a bias term for the correlation, which does
not exist in image processing.</p>

<p>Note that the anchor of the kernel is not at the center but at the top
left corner!</p>

<p>Refer to <a href="http://cs231n.github.io/convolutional-networks/">CS231N</a>.</p>

<p>For a color image, there are several kinds of filtering:</p>

<ul>
<li>concatenate the channels. For an rgb image, the filter kernel must be
<code>nxnx3</code>. For instance, if the filter kernel is <code>5x5x3</code>, for every position
<code>(x,y)</code>, take <code>5x5</code> from the red channel, <code>5x5</code> from the green channel,
<code>5x5</code> from the blue channel, and concatenate them into a vector
containing 75 elements and perform an inner product with the filter
to get an output. Therefore, after the filter operation, we get a gray image.
If we have another filter of size <code>5x5x3</code>, the above operation is repeated
and we get a 2-channel output. If we have <code>M</code> filters of size <code>5x5x3</code>, then
the final output is an image with <code>M</code> channels.</li>
</ul>

<h3 id="1d-convolution-in-pytorch">1D convolution in Pytorch</h3>

<p>Refer to <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d">torch.nn.Conv1d</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</pre></td></tr></table>
</div>
</div>
<h3 id="dilated-convolution">Dilated Convolution</h3>

<p>Refer to <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#dilated-convolution-animations">Dilated convolution animations</a>.</p>

<p>It is also known as atrous convolution.</p>

<ul>
<li>if we have 1 filter with kernel size 3x3

<ul>
<li>stride 0, pad 0, the output is</li>
</ul></li>
</ul>

<h1 id="cs231n-notes">CS231N Notes</h1>

<p><a href="http://cs231n.github.io/classification/">CS231n Convolutional Neural Networks for Visual Recognition</a></p>

<h2 id="image-classification-data-driven-approach-k-nearest-neighbor-train-val-test-splits">Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits</h2>

<p>See <a href="http://cs231n.github.io/classification/">http://cs231n.github.io/classification/</a></p>

<p><strong>Limitation</strong>: the number of categories of the images to be classified
<strong>MUST</strong> be predefined. The classification result is a probability map
and is not an exact prediction but the probability of being each category.
Even if you feed it with an image not in the predefined categories, it still
gives you a result!</p>

<p>topics covered:</p>

<ul>
<li>NN: nearest neighbor classification</li>
<li>kNN: k nearest neighbor classification</li>
<li>how to choose k in kNN

<ul>
<li>k is called a hyperparameter</li>
<li>split training set into a validation set and another training set</li>
<li>cross validation</li>
</ul></li>
</ul>

<h2 id="linear-classification-support-vector-machine-softmax">Linear classification: Support Vector Machine, Softmax</h2>

<p>See <a href="http://cs231n.github.io/linear-classify/">http://cs231n.github.io/linear-classify/</a></p>

<p>Topics covered:</p>

<ul>
<li>linear classifier as template matching</li>
<li>image data preprocessing</li>
<li>loss functions

<ul>
<li>SVM loss, hinge loss, squared hinge loss SVM (L2-SVM)</li>
</ul></li>
<li>regularization</li>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">cs229 SVM lecture notes</a>, primal and dual, KKT condition, SMO</li>
<li><a href="https://zhuanlan.zhihu.com/p/20599581">KKT</a> at zhihu.com</li>
</ul>

<h2 id="optimization-stochastic-gradient-descent">Optimization: Stochastic Gradient Descent</h2>

<p>See <a href="http://cs231n.github.io/optimization-1/">http://cs231n.github.io/optimization-1/</a></p>

<p>Topics covered:</p>

<ul>
<li>gradient descent</li>
<li>stochastic gradient descent (SGD)</li>
<li>batch stochastic gradient descent</li>
<li>compute gradient numerically</li>
</ul>

<h2 id="backpropagation-intuitions">Backpropagation, Intuitions</h2>

<p>See <a href="http://cs231n.github.io/optimization-2/">http://cs231n.github.io/optimization-2/</a></p>

<p>Topics covered:</p>

<ul>
<li>back propagation examples.</li>
</ul>

<h2 id="neural-networks-part-1-setting-up-the-architecture">Neural Networks Part 1: Setting up the Architecture</h2>

<p>See <a href="http://cs231n.github.io/neural-networks-1/">http://cs231n.github.io/neural-networks-1/</a></p>

<p>Topics covered:</p>

<ul>
<li>introduce the neural network in terms of matrix vector product!</li>
<li>different activation functions: sigmoid, tanh, ReLu, leaky ReLu, PReLu, MaxOut</li>
<li>how to count the number of parameters of a neural network</li>
</ul>

<h2 id="neural-networks-part-2-setting-up-the-data-and-the-loss">Neural Networks Part 2: Setting up the Data and the Loss</h2>

<p>See <a href="http://cs231n.github.io/neural-networks-2/">http://cs231n.github.io/neural-networks-2/</a></p>

<p>Topics covered:</p>

<ul>
<li>data normalization</li>
<li>weight initialization</li>
<li>weight regularization</li>
<li>batch normalization, drop out</li>
<li>classification loss: hinge loss, cross entropy loss</li>
<li>regression loss: L2</li>
</ul>

<p>Refer to <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/batch_norm_layer.cpp">batch_norm_layer.cpp</a>
and
<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/dropout_layer.cpp">dropout_layer.cpp</a> in caffe.
Note that caffe use inverted scale in drop out. The parameter
<a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/proto/caffe.proto#L698"><code>dropout_ratio</code></a> is the ratio to be removed. It is equal
to <code>1-p</code>, where <code>p</code> is the same meaning (i.e., present) as in the <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/solvers/adagrad_solver.cpp">paper</a>.</p>

<h2 id="neural-networks-part-3-learning-and-evaluation">Neural Networks Part 3: Learning and Evaluation</h2>

<p>See <a href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></p>

<p>Topics covered:</p>

<ul>
<li>tricks while training the network</li>
<li>gradient checker</li>
<li>how to determine the learning rate

<ul>
<li>loss curve</li>
<li>accuracy curves of training/test</li>
<li>the ratio $\frac{||\delta w||}{||w||}$ should be about 1e-3</li>
</ul></li>
<li>gradient descent

<ul>
<li>mementum update</li>
<li>neterov mementum</li>
<li>adagrad: adaptive subgradient, refer to the <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/solvers/adagrad_solver.cpp">caffe</a> implementation.</li>
</ul></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">history = history + gradient * gradient
detla_x = learning_rate * gradient / (sqrt(history) + eps)
x = x - delta_x</pre></td></tr></table>
</div>
</div>
<ul>
<li>RMSprop: refer to the <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/solvers/rmsprop_solver.cpp">caffe</a> implementation</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">history = momentum * history + (1-momentum) * gradient * gradient
detla_x = learning_rate * gradient / (sqrt(history) + eps)
x = x - delta_x</pre></td></tr></table>
</div>
</div>
<p>Note that <code>momentum</code> here is <a href="https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/src/caffe/proto/caffe.proto#L225"><code>rms_decay</code></a> in caffe</p>

<ul>
<li>model ensembles: train the network multiple times and average the output
of trained models</li>
</ul>

<p>Things to take way for this section:</p>

<ul>
<li>initial learning rate</li>
<li>how to decrease the learning rate along training</li>
<li>how to perform hyperparameter search</li>
</ul>

<h2 id="putting-it-together-minimal-neural-network-case-study">Putting it together: Minimal Neural Network Case Study</h2>

<p>See <a href="http://cs231n.github.io/neural-networks-case-study/">http://cs231n.github.io/neural-networks-case-study/</a></p>

<p>There is no convolution in this section!</p>

<p>It first implements a linear classifier with softmax cross entropy
loss to classifier 3 classes and finds that the accuracy rate is about 49%.
Then a hidden layer with 100 neuron is added and retrained, the final accuracy
is 98%.</p>

<p>There are 3 classes, each of which has 100 2-D points.</p>

<p>It does not show how to draw the decision boundary.</p>

<p>It is implemented in python and no framework is used.</p>

<h2 id="convolutional-neural-networks-architectures-convolution-pooling-layers">Convolutional Neural Networks: Architectures, Convolution / Pooling Layers</h2>

<p>See <a href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></p>

<p>For a color image of size <code>3x24x24</code> and the receptive field is of size <code>5x5</code>,
then the filter kernel is of size <code>3x5x5</code> and the output is <code>24x24</code>.</p>

<p>The anchor of the kernel is its top left element and is <strong>NOT</strong> at the center!</p>

<p>For an input of size <code>3x227x227</code> and 96 filters of size <code>3x11x11</code>, the number of
parameters is
$$
96\times 3 \times 11 times 11  + 96 = 34944
$$</p>

<p>We add 96 because every filter has additional bias, which is quiet different
from the image processing point of view!</p>

<p>Every filter forms a slice and pixels in this slice have the same filter, which
is called parameter sharing.</p>

<p>Note that this section uses actually cross-correlation,
but it considers the operation as convolution.</p>

<p>Pooling decreases the feature size resulting into fewer parameters, which
controls overfitting.</p>

<h3 id="layer-patterns">Layer Patterns</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">input -&gt; [conv -&gt;ReLU]*N -&gt; Pool?]*M -&gt; [FC -&gt; ReLU]*K -&gt; FC</pre></td></tr></table>
</div>
</div>
<p><code>Pool?</code> means pool is optional.</p>

<ul>
<li>0 &lt;= N &lt;= 3</li>
<li>M &gt;= 0</li>
<li>0 &lt;= K &lt; 3</li>
</ul>

<h4 id="lenet">LeNet</h4>

<p>see <a href="https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt">https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></pre></td>
<td class="lntd">
<pre class="chroma">input: 64x1x28x28

conv1:
    20 kernels, 5x5, stride 1, pad 0
    output size: 64x20x24x24

pool1 (max):
    2x2, stride 2, pad 0
    output size: 64x20x12x12

conv2:
    50 kernels, 5x5, stride 1, pad 0
    output size: 64x50x8x8

pool2 (max):
    2x2, stride 2, pad 0
    output size: 64x50x4x4

FC (inner product)
    500
    output size: 64x500

ReLU
    output size: 64x500

FC (inner product)
    10
    output size: 64x10

Softmax</pre></td></tr></table>
</div>
</div>
<h3 id="vgg-16">VGG 16</h3>

<p>refer to prototxt from <a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md">https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span></pre></td>
<td class="lntd">
<pre class="chroma">input: 3x224x224
conv1_1:
    64 filters, 3x3, pad 1, stride 1
    output size: 64x224x224
relu1_1:
    output size: 64x224x224
conv1_2:
    64 filters, 3x3, pad 1, stride 1
    output size: 64x224x224
relu1_2:
    output size: 64x224x224
pool1:
    max, 2x2, stride 2, pad 0
    output size: 64x112x112
conv2_1:
    128 filters, 3x3, pad 1, stride 1
    output size: 128x112x112
relu2_1:
    output size: 128x112x112
conv2_2:
    128 filters, 3x3, pad 1, stride 1
    output size: 128x112x112
relu2_2:
    output size: 128x112x112
pool2:
    max, 2x2, stride 2, pad 0
    output size: 128x56x56
conv3_1:
    256 filters, 3x3, pad 1, stride 1
    output size: 256x56x56
relu3_1:
    output size: 256x56x56
conv3_2:
    256 filters, 3x3, pad 1, stride 1
    output size: 256x56x56
relu3_2:
    output size: 256x56x56
conv3_3:
    256 filters, 3x3, pad 1, stride 1
    output size: 256x56x56
relu3_3:
    output size: 256x56x56
pool3:
    max, 2x2, stride 2, pad 0
    output size: 256x28x28
conv4_1:
    512 filters, 3x3, pad 1, stride 1
    output size: 512x28x28
relu4_1:
    output size: 512x28x28
conv4_2:
    512 filters, 3x3, pad 1, stride 1
    output size: 512x28x28
relu4_2:
    output size: 512x28x28
conv4_3:
    512 filters, 3x3, pad 1, stride 1
    output size: 512x28x28
relu4_3:
    output size: 512x28x28
pool4:
    max, 2x2, stride 2, pad 0
    output size: 512x14x14
conv5_1:
    512 filters, 3x3, pad 1, stride 1
    output size: 512x14x14
relu5_1:
    output size: 512x14x14
conv5_2:
    512 filters, 3x3, pad 1, stride 1
    output size: 512x14x14
relu5_2:
    output size: 512x14x14
conv5_3:
    512 filters, 3x3, pad 1, stride 1
    output size: 512x14x14
relu5_3:
    output size: 512x14x14
pool5:
    max, 2x2, stride 2, pad 0
    output size: 512x7x7
FC6:
    4096
    output size: 4096x1
relu6:
    output size: 4096
drop6:
    dropout ratio: 0.5
    output size: 4096
FC7:
    4096
    output size: 4096
relu7:
    output size: 4096
drop7:
    dropout ratio: 0.5
    output size: 4096
FC8:
    1000
    output size: 1000
prob (softmax)
    outptu size: 1000</pre></td></tr></table>
</div>
</div>
<p>Parameters</p>

<ul>
<li>conv1-1: 64 x 3x3x3 + 64 = 1802</li>
<li>conv1-2: 64 x 64x3x3 + 64 = 36928</li>
<li>conv2-1: 128 x 64x3x3 + 128 = 73856</li>
<li>conv2-2: 128 x 128x3x3 + 128 = 147584</li>
<li>conv3-1: 256 x 128x3x3 + 256 = 294912</li>
<li>conv3-2: 256 x 256x3x3 + 256 = 590080</li>
<li>conv3-3: 256 x 256x3x3 + 256 = 590080</li>
<li>conv4-1: 512 x 256x3x3 + 512 = 1180160</li>
<li>conv4-2: 512 x 512x3x3 + 512 = 2359808</li>
<li>conv4-3: 512 x 512x3x3 + 512 = 2359808</li>
<li>conv5-1: 512 x 512x3x3 + 512 = 2359808</li>
<li>conv5-2: 512 x 512x3x3 + 512 = 2359808</li>
<li>conv5-3: 512 x 512x3x3 + 512 = 2359808</li>
<li>FC6: 4096*512*7*7 + 4096     = 102764544</li>
<li>FC7: 4096*4096+4096          = 16781312</li>
<li>FC8: 1000*4096+1000          = 4097000</li>
</ul>

<p>The above parameters do not contain spaces for gradient update
and momentum update.</p>

<h1 id="datasets">Datasets</h1>

<ul>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html">The CIFAR-10 dataset</a>, CIFAR-10 - Object Recognition in Images at <a href="https://www.kaggle.com/c/cifar-10">kaggle</a></li>
</ul>

<h1 id="todo">todo</h1>

<p>AlexNet, GoogleNet, ResNet, VGG, Inception</p>

<p>dropout, batch normalization, pooling</p>

<p>sgd</p>

<p><a href="http://cs230.stanford.edu/index.html">CS230: Deep Learning</a></p>

<p><a href="http://www.cs.toronto.edu/~tijmen/csc321/information.shtml">CSC321 Winter 2014 Introduction to Neural Networks and Machine Learning</a>
by Geoffrey Hinton</p>

<h1 id="references">References</h1>

<ul>
<li><a href="http://shengshuyang.github.io/A-step-by-step-guide-to-Caffe.html">A step by step guide to Caffe</a></li>
<li><a href="http://caffe.berkeleyvision.org/tutorial/layers.html">Layers</a></li>
<li><a href="https://arxiv.org/pdf/1603.07285.pdf">A guide to convolution arithmetic for deep learning</a>, a paper, 2018, arxiv.org</li>
<li><a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html">Blobs, Layers, and Nets: anatomy of a Caffe model</a></li>
<li><a href="http://cs231n.github.io/neural-networks-case-study/">http://cs231n.github.io/neural-networks-case-study/</a></li>
</ul>

    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">fangjun</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2018-11-02</span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>

    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://csu-fangjun.github.io/tags/caffe/">caffe</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/post/image-processing-in-go/">
            <span class="next-text nav-default">Image Processing in Go</span>
            <span class="prev-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  

  <div class="disqus-comment">
  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
    Show Disqus Comments
  </div>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_config = function () {
      this.page.url = "https://csu-fangjun.github.io/post/caffe-notes/";
    };
    function load_disqus() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'csu-fangjun';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      $('#load_disqus').remove();
    };
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
  
  </div>

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:fangjun.kuang@gmail.com" rel="me noopener" class="iconfont icon-email"
        title="email">
      </a>
      <a href="https://github.com/csu-fangjun" rel="me noopener" class="iconfont icon-github"
        title="github" target="_blank">
      </a>
  <a href="https://csu-fangjun.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml" class="iconfont icon-rss"
    title="rss" target="_blank">
  </a>
  </div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span><span class="author">
        fangjun
        
      </span></span>

  
  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
<script type="text/javascript" src="/dist/jane.min.js?v=2.7.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>





  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  











</body>
</html>
